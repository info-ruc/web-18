{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import hashlib\n",
    "\n",
    "def _unzip(save_path, _, database_name, data_path):\n",
    "    \"\"\"\n",
    "    Unzip wrapper with the same interface as _ungzip\n",
    "    :param save_path: The path of the gzip files\n",
    "    :param database_name: Name of database\n",
    "    :param data_path: Path to extract to\n",
    "    :param _: HACK - Used to have to same interface as _ungzip\n",
    "    \"\"\"\n",
    "    print('Extracting {}...'.format(database_name))\n",
    "    with zipfile.ZipFile(save_path) as zf:\n",
    "        zf.extractall(data_path)\n",
    "\n",
    "def download_extract(database_name, data_path):\n",
    "    \"\"\"\n",
    "    Download and extract database\n",
    "    :param database_name: Database name\n",
    "    \"\"\"\n",
    "    DATASET_ML1M = 'ml-1m'\n",
    "\n",
    "    if database_name == DATASET_ML1M:\n",
    "        url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n",
    "        hash_code = 'c4d9eecfca2ab87c1945afe126590906'\n",
    "        extract_path = os.path.join(data_path, 'ml-1m')\n",
    "        save_path = os.path.join(data_path, 'ml-1m.zip')\n",
    "        extract_fn = _unzip\n",
    "\n",
    "    if os.path.exists(extract_path):\n",
    "        print('Found {} Data'.format(database_name))\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Downloading {}'.format(database_name)) as pbar:\n",
    "            urlretrieve(\n",
    "                url,\n",
    "                save_path,\n",
    "                pbar.hook)\n",
    "\n",
    "    assert hashlib.md5(open(save_path, 'rb').read()).hexdigest() == hash_code, \\\n",
    "        '{} file is corrupted.  Remove the file and try again.'.format(save_path)\n",
    "\n",
    "    os.makedirs(extract_path)\n",
    "    try:\n",
    "        extract_fn(save_path, extract_path, database_name, data_path)\n",
    "    except Exception as err:\n",
    "        shutil.rmtree(extract_path)  # Remove extraction folder if there is an error\n",
    "        raise err\n",
    "\n",
    "    print('Done.')\n",
    "    # Remove compressed data\n",
    "#     os.remove(save_path)\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    \"\"\"\n",
    "    Handle Progress Bar while Downloading\n",
    "    \"\"\"\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        \"\"\"\n",
    "        A hook function that will be called once on establishment of the network connection and\n",
    "        once after each block read thereafter.\n",
    "        :param block_num: A count of blocks transferred so far\n",
    "        :param block_size: Block size in bytes\n",
    "        :param total_size: The total size of the file. This may be -1 on older FTP servers which do not return\n",
    "                            a file size in response to a retrieval request.\n",
    "        \"\"\"\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading ml-1m: 5.92MB [01:37, 60.6kB/s]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ml-1m...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = './'\n",
    "download_extract('ml-1m', data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    #读取User数据\n",
    "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "    users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "    users_orig = users.values\n",
    "    #改变User数据中性别和年龄\n",
    "    gender_map = {'F':0, 'M':1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)\n",
    "\n",
    "    age_map = {val:ii for ii,val in enumerate(set(users['Age']))}\n",
    "    users['Age'] = users['Age'].map(age_map)\n",
    "\n",
    "    #读取Movie数据集\n",
    "    movies_title = ['MovieID', 'Title', 'Genres']\n",
    "    movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "    movies_orig = movies.values\n",
    "    #将Title中的年份去掉\n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    "\n",
    "    title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #电影类型转数字字典\n",
    "    genres_set = set()\n",
    "    for val in movies['Genres'].str.split('|'):\n",
    "        genres_set.update(val)\n",
    "\n",
    "    genres_set.add('<PAD>')\n",
    "    genres2int = {val:ii for ii, val in enumerate(genres_set)}\n",
    "\n",
    "    #将电影类型转成等长数字列表，长度是18\n",
    "    genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "\n",
    "    for key in genres_map:\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "            genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])\n",
    "    \n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "\n",
    "    #电影Title转数字字典\n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():\n",
    "        title_set.update(val)\n",
    "    \n",
    "    title_set.add('<PAD>')\n",
    "    title2int = {val:ii for ii, val in enumerate(title_set)}\n",
    "\n",
    "    #将电影Title转成等长数字列表，长度是15\n",
    "    title_count = 15\n",
    "    title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
    "    \n",
    "    for key in title_map:\n",
    "        for cnt in range(title_count - len(title_map[key])):\n",
    "            title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
    "    \n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #读取评分数据集\n",
    "    ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "\n",
    "    #合并三个表\n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "    \n",
    "    #将数据分成X和y两张表\n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    \n",
    "    features = features_pd.values\n",
    "    targets_values = targets_pd.values\n",
    "    \n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "\n",
    "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[4286, 1753, 491, 491, 491, 491, 491, 491, 491...</td>\n",
       "      <td>[10, 14, 15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1041, 491, 491, 491, 491, 491, 491, 491, 491,...</td>\n",
       "      <td>[12, 14, 13, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[5057, 388, 3991, 491, 491, 491, 491, 491, 491...</td>\n",
       "      <td>[15, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[4240, 3630, 253, 491, 491, 491, 491, 491, 491...</td>\n",
       "      <td>[15, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[4207, 2690, 2682, 2246, 1441, 2618, 491, 491,...</td>\n",
       "      <td>[15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [4286, 1753, 491, 491, 491, 491, 491, 491, 491...   \n",
       "1        2  [1041, 491, 491, 491, 491, 491, 491, 491, 491,...   \n",
       "2        3  [5057, 388, 3991, 491, 491, 491, 491, 491, 491...   \n",
       "3        4  [4240, 3630, 253, 491, 491, 491, 491, 491, 491...   \n",
       "4        5  [4207, 2690, 2682, 2246, 1441, 2618, 491, 491,...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [10, 14, 15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "1  [12, 14, 13, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "2  [15, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,...  \n",
       "3  [15, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,...  \n",
       "4  [15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       list([4286, 1753, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491]),\n",
       "       list([10, 14, 15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "#用户ID个数\n",
    "uid_max = max(features.take(0,1)) + 1 # 6040\n",
    "#性别个数\n",
    "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
    "#年龄类别个数\n",
    "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
    "#职业个数\n",
    "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
    "\n",
    "#电影ID个数\n",
    "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
    "#电影类型个数\n",
    "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
    "#电影名单词个数\n",
    "movie_title_max = len(title_set) # 5216\n",
    "\n",
    "#对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
    "combiner = \"sum\"\n",
    "\n",
    "#电影名长度\n",
    "sentences_size = title_count # = 15\n",
    "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "dropout_keep = 0.5\n",
    "# Learning Rate\n",
    "learning_rate = 0.0001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 20\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name=\"uid\")\n",
    "    user_gender = tf.placeholder(tf.int32, [None, 1], name=\"user_gender\")\n",
    "    user_age = tf.placeholder(tf.int32, [None, 1], name=\"user_age\")\n",
    "    user_job = tf.placeholder(tf.int32, [None, 1], name=\"user_job\")\n",
    "    \n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name=\"movie_id\")\n",
    "    movie_categories = tf.placeholder(tf.int32, [None, 18], name=\"movie_categories\")\n",
    "    movie_titles = tf.placeholder(tf.int32, [None, 15], name=\"movie_titles\")\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    with tf.name_scope(\"user_embedding\"):\n",
    "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name = \"uid_embed_matrix\")\n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name = \"uid_embed_layer\")\n",
    "    \n",
    "        gender_embed_matrix = tf.Variable(tf.random_uniform([gender_max, embed_dim // 2], -1, 1), name= \"gender_embed_matrix\")\n",
    "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name = \"gender_embed_layer\")\n",
    "        \n",
    "        age_embed_matrix = tf.Variable(tf.random_uniform([age_max, embed_dim // 2], -1, 1), name=\"age_embed_matrix\")\n",
    "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name=\"age_embed_layer\")\n",
    "        \n",
    "        job_embed_matrix = tf.Variable(tf.random_uniform([job_max, embed_dim // 2], -1, 1), name = \"job_embed_matrix\")\n",
    "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name = \"job_embed_layer\")\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    with tf.name_scope(\"user_fc\"):\n",
    "        #第一层全连接\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name = \"uid_fc_layer\", activation=tf.nn.relu)\n",
    "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name = \"gender_fc_layer\", activation=tf.nn.relu)\n",
    "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name =\"age_fc_layer\", activation=tf.nn.relu)\n",
    "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name = \"job_fc_layer\", activation=tf.nn.relu)\n",
    "        \n",
    "        #第二层全连接\n",
    "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
    "        user_combine_layer = tf.contrib.layers.fully_connected(user_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name = \"movie_id_embed_matrix\")\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name = \"movie_id_embed_layer\")\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    with tf.name_scope(\"movie_categories_layers\"):\n",
    "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1), name = \"movie_categories_embed_matrix\")\n",
    "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories, name = \"movie_categories_embed_layer\")\n",
    "        if combiner == \"sum\":\n",
    "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True)\n",
    "    #     elif combiner == \"mean\":\n",
    "\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_title_embed_matrix = tf.Variable(tf.random_uniform([movie_title_max, embed_dim], -1, 1), name = \"movie_title_embed_matrix\")\n",
    "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_titles, name = \"movie_title_embed_layer\")\n",
    "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1)\n",
    "    \n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        with tf.name_scope(\"movie_txt_conv_maxpool_{}\".format(window_size)):\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num],stddev=0.1),name = \"filter_weights\")\n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"filter_bias\")\n",
    "            \n",
    "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1], padding=\"VALID\", name=\"conv_layer\")\n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer,filter_bias), name =\"relu_layer\")\n",
    "            \n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1], padding=\"VALID\", name=\"maxpool_layer\")\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "\n",
    "    #Dropout层\n",
    "    with tf.name_scope(\"pool_dropout\"):\n",
    "        pool_layer = tf.concat(pool_layer_lst, 3, name =\"pool_layer\")\n",
    "        max_num = len(window_sizes) * filter_num\n",
    "        pool_layer_flat = tf.reshape(pool_layer , [-1, 1, max_num], name = \"pool_layer_flat\")\n",
    "    \n",
    "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name = \"dropout_layer\")\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    with tf.name_scope(\"movie_fc\"):\n",
    "        #第一层全连接\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name = \"movie_id_fc_layer\", activation=tf.nn.relu)\n",
    "        movie_categories_fc_layer = tf.layers.dense(movie_categories_embed_layer, embed_dim, name = \"movie_categories_fc_layer\", activation=tf.nn.relu)\n",
    "    \n",
    "        #第二层全连接\n",
    "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  #(?, 1, 96)\n",
    "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-559a1ee9ce9e>:6: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    #获取输入占位符\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    #获取User的4个嵌入向量\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
    "    #得到用户特征\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
    "    #获取电影ID的嵌入向量\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "    #获取电影类型的嵌入向量\n",
    "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "    #获取电影名的特征向量\n",
    "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "    #得到电影特征\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, \n",
    "                                                                                movie_categories_embed_layer, \n",
    "                                                                                dropout_layer)\n",
    "    #计算出评分，要注意两个不同的方案，inference的名字（name值）是不一样的，后面做推荐时要根据name取得tensor\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        #将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "#         inference_layer = tf.concat([user_combine_layer_flat, movie_combine_layer_flat], 1)  #(?, 200)\n",
    "#         inference = tf.layers.dense(inference_layer, 1,\n",
    "#                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.01), \n",
    "#                                     kernel_regularizer=tf.nn.l2_loss, name=\"inference\")\n",
    "        #简单的将用户特征和电影特征做矩阵乘法得到一个预测评分\n",
    "#        inference = tf.matmul(user_combine_layer_flat, tf.transpose(movie_combine_layer_flat))\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)\n",
    "        inference = tf.expand_dims(inference, axis=1)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        cost = tf.losses.mean_squared_error(targets, inference )\n",
    "        loss = tf.reduce_mean(cost)\n",
    "    # 优化损失 \n",
    "#     train_op = tf.train.AdamOptimizer(lr).minimize(loss)  #cost\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'inference/ExpandDims:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /notebooks/runs/1528389562\n",
      "\n",
      "2018-06-07T16:39:25.245309: Epoch   0 Batch    0/3125   train_loss = 5.090\n",
      "2018-06-07T16:39:25.982468: Epoch   0 Batch   20/3125   train_loss = 2.394\n",
      "2018-06-07T16:39:26.667177: Epoch   0 Batch   40/3125   train_loss = 2.066\n",
      "2018-06-07T16:39:27.357190: Epoch   0 Batch   60/3125   train_loss = 1.617\n",
      "2018-06-07T16:39:28.060043: Epoch   0 Batch   80/3125   train_loss = 1.880\n",
      "2018-06-07T16:39:28.804709: Epoch   0 Batch  100/3125   train_loss = 1.664\n",
      "2018-06-07T16:39:29.502068: Epoch   0 Batch  120/3125   train_loss = 1.727\n",
      "2018-06-07T16:39:30.195836: Epoch   0 Batch  140/3125   train_loss = 1.547\n",
      "2018-06-07T16:39:30.941947: Epoch   0 Batch  160/3125   train_loss = 1.340\n",
      "2018-06-07T16:39:31.689727: Epoch   0 Batch  180/3125   train_loss = 1.397\n",
      "2018-06-07T16:39:32.394647: Epoch   0 Batch  200/3125   train_loss = 1.563\n",
      "2018-06-07T16:39:33.134539: Epoch   0 Batch  220/3125   train_loss = 1.416\n",
      "2018-06-07T16:39:33.900162: Epoch   0 Batch  240/3125   train_loss = 1.363\n",
      "2018-06-07T16:39:34.733104: Epoch   0 Batch  260/3125   train_loss = 1.443\n",
      "2018-06-07T16:39:35.465186: Epoch   0 Batch  280/3125   train_loss = 1.608\n",
      "2018-06-07T16:39:36.203587: Epoch   0 Batch  300/3125   train_loss = 1.528\n",
      "2018-06-07T16:39:36.921248: Epoch   0 Batch  320/3125   train_loss = 1.424\n",
      "2018-06-07T16:39:37.651434: Epoch   0 Batch  340/3125   train_loss = 1.378\n",
      "2018-06-07T16:39:38.459963: Epoch   0 Batch  360/3125   train_loss = 1.366\n",
      "2018-06-07T16:39:39.180430: Epoch   0 Batch  380/3125   train_loss = 1.335\n",
      "2018-06-07T16:39:40.047664: Epoch   0 Batch  400/3125   train_loss = 1.150\n",
      "2018-06-07T16:39:40.798695: Epoch   0 Batch  420/3125   train_loss = 1.276\n",
      "2018-06-07T16:39:41.571728: Epoch   0 Batch  440/3125   train_loss = 1.493\n",
      "2018-06-07T16:39:42.374370: Epoch   0 Batch  460/3125   train_loss = 1.209\n",
      "2018-06-07T16:39:43.188142: Epoch   0 Batch  480/3125   train_loss = 1.291\n",
      "2018-06-07T16:39:43.961391: Epoch   0 Batch  500/3125   train_loss = 1.003\n",
      "2018-06-07T16:39:44.647086: Epoch   0 Batch  520/3125   train_loss = 1.377\n",
      "2018-06-07T16:39:45.356655: Epoch   0 Batch  540/3125   train_loss = 1.245\n",
      "2018-06-07T16:39:46.256768: Epoch   0 Batch  560/3125   train_loss = 1.373\n",
      "2018-06-07T16:39:46.962244: Epoch   0 Batch  580/3125   train_loss = 1.414\n",
      "2018-06-07T16:39:47.864005: Epoch   0 Batch  600/3125   train_loss = 1.395\n",
      "2018-06-07T16:39:48.644075: Epoch   0 Batch  620/3125   train_loss = 1.429\n",
      "2018-06-07T16:39:49.381981: Epoch   0 Batch  640/3125   train_loss = 1.340\n",
      "2018-06-07T16:39:50.109624: Epoch   0 Batch  660/3125   train_loss = 1.293\n",
      "2018-06-07T16:39:50.841366: Epoch   0 Batch  680/3125   train_loss = 1.238\n",
      "2018-06-07T16:39:51.553317: Epoch   0 Batch  700/3125   train_loss = 1.273\n",
      "2018-06-07T16:39:52.225101: Epoch   0 Batch  720/3125   train_loss = 1.159\n",
      "2018-06-07T16:39:52.943986: Epoch   0 Batch  740/3125   train_loss = 1.393\n",
      "2018-06-07T16:39:53.644627: Epoch   0 Batch  760/3125   train_loss = 1.366\n",
      "2018-06-07T16:39:54.315532: Epoch   0 Batch  780/3125   train_loss = 1.376\n",
      "2018-06-07T16:39:55.027939: Epoch   0 Batch  800/3125   train_loss = 1.249\n",
      "2018-06-07T16:39:55.757416: Epoch   0 Batch  820/3125   train_loss = 1.227\n",
      "2018-06-07T16:39:56.474828: Epoch   0 Batch  840/3125   train_loss = 1.147\n",
      "2018-06-07T16:39:57.291220: Epoch   0 Batch  860/3125   train_loss = 1.146\n",
      "2018-06-07T16:39:58.053681: Epoch   0 Batch  880/3125   train_loss = 1.174\n",
      "2018-06-07T16:39:58.885159: Epoch   0 Batch  900/3125   train_loss = 1.222\n",
      "2018-06-07T16:39:59.574032: Epoch   0 Batch  920/3125   train_loss = 1.245\n",
      "2018-06-07T16:40:00.396765: Epoch   0 Batch  940/3125   train_loss = 1.511\n",
      "2018-06-07T16:40:01.297871: Epoch   0 Batch  960/3125   train_loss = 1.309\n",
      "2018-06-07T16:40:02.064070: Epoch   0 Batch  980/3125   train_loss = 1.347\n",
      "2018-06-07T16:40:02.869704: Epoch   0 Batch 1000/3125   train_loss = 1.260\n",
      "2018-06-07T16:40:03.617357: Epoch   0 Batch 1020/3125   train_loss = 1.325\n",
      "2018-06-07T16:40:04.321179: Epoch   0 Batch 1040/3125   train_loss = 1.298\n",
      "2018-06-07T16:40:05.025824: Epoch   0 Batch 1060/3125   train_loss = 1.485\n",
      "2018-06-07T16:40:05.791506: Epoch   0 Batch 1080/3125   train_loss = 1.200\n",
      "2018-06-07T16:40:06.527857: Epoch   0 Batch 1100/3125   train_loss = 1.298\n",
      "2018-06-07T16:40:07.238561: Epoch   0 Batch 1120/3125   train_loss = 1.267\n",
      "2018-06-07T16:40:07.964544: Epoch   0 Batch 1140/3125   train_loss = 1.353\n",
      "2018-06-07T16:40:08.713746: Epoch   0 Batch 1160/3125   train_loss = 1.263\n",
      "2018-06-07T16:40:09.430777: Epoch   0 Batch 1180/3125   train_loss = 1.210\n",
      "2018-06-07T16:40:10.118158: Epoch   0 Batch 1200/3125   train_loss = 1.206\n",
      "2018-06-07T16:40:10.821353: Epoch   0 Batch 1220/3125   train_loss = 1.125\n",
      "2018-06-07T16:40:11.525611: Epoch   0 Batch 1240/3125   train_loss = 1.129\n",
      "2018-06-07T16:40:12.219610: Epoch   0 Batch 1260/3125   train_loss = 1.189\n",
      "2018-06-07T16:40:12.902863: Epoch   0 Batch 1280/3125   train_loss = 1.187\n",
      "2018-06-07T16:40:13.581626: Epoch   0 Batch 1300/3125   train_loss = 1.238\n",
      "2018-06-07T16:40:14.262076: Epoch   0 Batch 1320/3125   train_loss = 1.122\n",
      "2018-06-07T16:40:14.944195: Epoch   0 Batch 1340/3125   train_loss = 1.049\n",
      "2018-06-07T16:40:15.665754: Epoch   0 Batch 1360/3125   train_loss = 1.163\n",
      "2018-06-07T16:40:16.351840: Epoch   0 Batch 1380/3125   train_loss = 1.030\n",
      "2018-06-07T16:40:17.045353: Epoch   0 Batch 1400/3125   train_loss = 1.239\n",
      "2018-06-07T16:40:17.727338: Epoch   0 Batch 1420/3125   train_loss = 1.337\n",
      "2018-06-07T16:40:18.436297: Epoch   0 Batch 1440/3125   train_loss = 1.244\n",
      "2018-06-07T16:40:19.122342: Epoch   0 Batch 1460/3125   train_loss = 1.161\n",
      "2018-06-07T16:40:19.801029: Epoch   0 Batch 1480/3125   train_loss = 1.212\n",
      "2018-06-07T16:40:20.503928: Epoch   0 Batch 1500/3125   train_loss = 1.341\n",
      "2018-06-07T16:40:21.188031: Epoch   0 Batch 1520/3125   train_loss = 1.232\n",
      "2018-06-07T16:40:21.878181: Epoch   0 Batch 1540/3125   train_loss = 1.237\n",
      "2018-06-07T16:40:22.589612: Epoch   0 Batch 1560/3125   train_loss = 1.152\n",
      "2018-06-07T16:40:23.327554: Epoch   0 Batch 1580/3125   train_loss = 1.207\n",
      "2018-06-07T16:40:24.130113: Epoch   0 Batch 1600/3125   train_loss = 1.194\n",
      "2018-06-07T16:40:24.876765: Epoch   0 Batch 1620/3125   train_loss = 1.142\n",
      "2018-06-07T16:40:25.575841: Epoch   0 Batch 1640/3125   train_loss = 1.261\n",
      "2018-06-07T16:40:26.272416: Epoch   0 Batch 1660/3125   train_loss = 1.281\n",
      "2018-06-07T16:40:26.974668: Epoch   0 Batch 1680/3125   train_loss = 1.179\n",
      "2018-06-07T16:40:27.672854: Epoch   0 Batch 1700/3125   train_loss = 1.074\n",
      "2018-06-07T16:40:28.389697: Epoch   0 Batch 1720/3125   train_loss = 1.180\n",
      "2018-06-07T16:40:29.127294: Epoch   0 Batch 1740/3125   train_loss = 1.191\n",
      "2018-06-07T16:40:29.820889: Epoch   0 Batch 1760/3125   train_loss = 1.355\n",
      "2018-06-07T16:40:30.517096: Epoch   0 Batch 1780/3125   train_loss = 1.077\n",
      "2018-06-07T16:40:31.203643: Epoch   0 Batch 1800/3125   train_loss = 1.132\n",
      "2018-06-07T16:40:31.892082: Epoch   0 Batch 1820/3125   train_loss = 1.149\n",
      "2018-06-07T16:40:32.618120: Epoch   0 Batch 1840/3125   train_loss = 1.258\n",
      "2018-06-07T16:40:33.274190: Epoch   0 Batch 1860/3125   train_loss = 1.277\n",
      "2018-06-07T16:40:34.056723: Epoch   0 Batch 1880/3125   train_loss = 1.256\n",
      "2018-06-07T16:40:34.816073: Epoch   0 Batch 1900/3125   train_loss = 1.061\n",
      "2018-06-07T16:40:35.480970: Epoch   0 Batch 1920/3125   train_loss = 1.131\n",
      "2018-06-07T16:40:36.158612: Epoch   0 Batch 1940/3125   train_loss = 1.108\n",
      "2018-06-07T16:40:36.825617: Epoch   0 Batch 1960/3125   train_loss = 1.161\n",
      "2018-06-07T16:40:37.493528: Epoch   0 Batch 1980/3125   train_loss = 1.147\n",
      "2018-06-07T16:40:38.163617: Epoch   0 Batch 2000/3125   train_loss = 1.433\n",
      "2018-06-07T16:40:38.852903: Epoch   0 Batch 2020/3125   train_loss = 1.261\n",
      "2018-06-07T16:40:39.543618: Epoch   0 Batch 2040/3125   train_loss = 1.063\n",
      "2018-06-07T16:40:40.224568: Epoch   0 Batch 2060/3125   train_loss = 1.031\n",
      "2018-06-07T16:40:40.950346: Epoch   0 Batch 2080/3125   train_loss = 1.229\n",
      "2018-06-07T16:40:41.694038: Epoch   0 Batch 2100/3125   train_loss = 1.093\n",
      "2018-06-07T16:40:42.444686: Epoch   0 Batch 2120/3125   train_loss = 1.043\n",
      "2018-06-07T16:40:43.316191: Epoch   0 Batch 2140/3125   train_loss = 1.204\n",
      "2018-06-07T16:40:44.009688: Epoch   0 Batch 2160/3125   train_loss = 1.144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-07T16:40:44.740460: Epoch   0 Batch 2180/3125   train_loss = 1.139\n",
      "2018-06-07T16:40:45.424969: Epoch   0 Batch 2200/3125   train_loss = 1.095\n",
      "2018-06-07T16:40:46.103753: Epoch   0 Batch 2220/3125   train_loss = 1.082\n",
      "2018-06-07T16:40:46.768109: Epoch   0 Batch 2240/3125   train_loss = 0.991\n",
      "2018-06-07T16:40:47.429826: Epoch   0 Batch 2260/3125   train_loss = 1.143\n",
      "2018-06-07T16:40:48.113688: Epoch   0 Batch 2280/3125   train_loss = 1.153\n",
      "2018-06-07T16:40:48.781053: Epoch   0 Batch 2300/3125   train_loss = 1.148\n",
      "2018-06-07T16:40:49.448221: Epoch   0 Batch 2320/3125   train_loss = 1.300\n",
      "2018-06-07T16:40:50.097550: Epoch   0 Batch 2340/3125   train_loss = 1.224\n",
      "2018-06-07T16:40:50.770005: Epoch   0 Batch 2360/3125   train_loss = 1.178\n",
      "2018-06-07T16:40:51.447653: Epoch   0 Batch 2380/3125   train_loss = 1.076\n",
      "2018-06-07T16:40:52.116176: Epoch   0 Batch 2400/3125   train_loss = 1.254\n",
      "2018-06-07T16:40:52.882825: Epoch   0 Batch 2420/3125   train_loss = 1.118\n",
      "2018-06-07T16:40:53.536109: Epoch   0 Batch 2440/3125   train_loss = 1.186\n",
      "2018-06-07T16:40:54.207191: Epoch   0 Batch 2460/3125   train_loss = 1.058\n",
      "2018-06-07T16:40:54.868490: Epoch   0 Batch 2480/3125   train_loss = 1.185\n",
      "2018-06-07T16:40:55.539685: Epoch   0 Batch 2500/3125   train_loss = 1.254\n",
      "2018-06-07T16:40:56.191188: Epoch   0 Batch 2520/3125   train_loss = 1.036\n",
      "2018-06-07T16:40:56.857810: Epoch   0 Batch 2540/3125   train_loss = 1.026\n",
      "2018-06-07T16:40:57.516035: Epoch   0 Batch 2560/3125   train_loss = 1.012\n",
      "2018-06-07T16:40:58.188383: Epoch   0 Batch 2580/3125   train_loss = 1.135\n",
      "2018-06-07T16:40:58.849670: Epoch   0 Batch 2600/3125   train_loss = 1.150\n",
      "2018-06-07T16:40:59.577893: Epoch   0 Batch 2620/3125   train_loss = 1.052\n",
      "2018-06-07T16:41:00.254619: Epoch   0 Batch 2640/3125   train_loss = 1.096\n",
      "2018-06-07T16:41:00.924452: Epoch   0 Batch 2660/3125   train_loss = 1.177\n",
      "2018-06-07T16:41:01.616235: Epoch   0 Batch 2680/3125   train_loss = 0.978\n",
      "2018-06-07T16:41:02.266099: Epoch   0 Batch 2700/3125   train_loss = 1.122\n",
      "2018-06-07T16:41:02.940846: Epoch   0 Batch 2720/3125   train_loss = 1.075\n",
      "2018-06-07T16:41:03.610625: Epoch   0 Batch 2740/3125   train_loss = 1.157\n",
      "2018-06-07T16:41:04.277482: Epoch   0 Batch 2760/3125   train_loss = 1.092\n",
      "2018-06-07T16:41:04.936146: Epoch   0 Batch 2780/3125   train_loss = 1.106\n",
      "2018-06-07T16:41:05.609268: Epoch   0 Batch 2800/3125   train_loss = 1.238\n",
      "2018-06-07T16:41:06.257802: Epoch   0 Batch 2820/3125   train_loss = 1.406\n",
      "2018-06-07T16:41:06.918270: Epoch   0 Batch 2840/3125   train_loss = 1.139\n",
      "2018-06-07T16:41:07.595942: Epoch   0 Batch 2860/3125   train_loss = 1.102\n",
      "2018-06-07T16:41:08.272044: Epoch   0 Batch 2880/3125   train_loss = 1.105\n",
      "2018-06-07T16:41:08.916191: Epoch   0 Batch 2900/3125   train_loss = 1.181\n",
      "2018-06-07T16:41:09.583556: Epoch   0 Batch 2920/3125   train_loss = 1.183\n",
      "2018-06-07T16:41:10.245900: Epoch   0 Batch 2940/3125   train_loss = 1.114\n",
      "2018-06-07T16:41:10.918331: Epoch   0 Batch 2960/3125   train_loss = 1.195\n",
      "2018-06-07T16:41:11.584714: Epoch   0 Batch 2980/3125   train_loss = 1.210\n",
      "2018-06-07T16:41:12.244710: Epoch   0 Batch 3000/3125   train_loss = 1.124\n",
      "2018-06-07T16:41:12.906493: Epoch   0 Batch 3020/3125   train_loss = 1.215\n",
      "2018-06-07T16:41:13.597725: Epoch   0 Batch 3040/3125   train_loss = 1.082\n",
      "2018-06-07T16:41:14.258515: Epoch   0 Batch 3060/3125   train_loss = 1.111\n",
      "2018-06-07T16:41:14.914082: Epoch   0 Batch 3080/3125   train_loss = 1.202\n",
      "2018-06-07T16:41:15.568733: Epoch   0 Batch 3100/3125   train_loss = 1.235\n",
      "2018-06-07T16:41:16.236088: Epoch   0 Batch 3120/3125   train_loss = 0.982\n",
      "2018-06-07T16:41:16.442698: Epoch   0 Batch    0/781   test_loss = 1.022\n",
      "2018-06-07T16:41:16.606331: Epoch   0 Batch   20/781   test_loss = 1.101\n",
      "2018-06-07T16:41:16.769408: Epoch   0 Batch   40/781   test_loss = 1.040\n",
      "2018-06-07T16:41:16.914933: Epoch   0 Batch   60/781   test_loss = 1.227\n",
      "2018-06-07T16:41:17.083129: Epoch   0 Batch   80/781   test_loss = 1.289\n",
      "2018-06-07T16:41:17.236560: Epoch   0 Batch  100/781   test_loss = 1.298\n",
      "2018-06-07T16:41:17.391790: Epoch   0 Batch  120/781   test_loss = 1.196\n",
      "2018-06-07T16:41:17.537591: Epoch   0 Batch  140/781   test_loss = 1.105\n",
      "2018-06-07T16:41:17.702658: Epoch   0 Batch  160/781   test_loss = 1.267\n",
      "2018-06-07T16:41:17.853037: Epoch   0 Batch  180/781   test_loss = 1.179\n",
      "2018-06-07T16:41:18.001179: Epoch   0 Batch  200/781   test_loss = 1.164\n",
      "2018-06-07T16:41:18.144141: Epoch   0 Batch  220/781   test_loss = 0.941\n",
      "2018-06-07T16:41:18.305898: Epoch   0 Batch  240/781   test_loss = 1.160\n",
      "2018-06-07T16:41:18.444444: Epoch   0 Batch  260/781   test_loss = 1.082\n",
      "2018-06-07T16:41:18.595585: Epoch   0 Batch  280/781   test_loss = 1.375\n",
      "2018-06-07T16:41:18.737131: Epoch   0 Batch  300/781   test_loss = 1.131\n",
      "2018-06-07T16:41:18.895345: Epoch   0 Batch  320/781   test_loss = 1.205\n",
      "2018-06-07T16:41:19.036722: Epoch   0 Batch  340/781   test_loss = 0.875\n",
      "2018-06-07T16:41:19.190663: Epoch   0 Batch  360/781   test_loss = 1.264\n",
      "2018-06-07T16:41:19.340135: Epoch   0 Batch  380/781   test_loss = 1.180\n",
      "2018-06-07T16:41:19.491343: Epoch   0 Batch  400/781   test_loss = 1.055\n",
      "2018-06-07T16:41:19.640334: Epoch   0 Batch  420/781   test_loss = 0.974\n",
      "2018-06-07T16:41:19.792399: Epoch   0 Batch  440/781   test_loss = 1.122\n",
      "2018-06-07T16:41:19.948918: Epoch   0 Batch  460/781   test_loss = 1.093\n",
      "2018-06-07T16:41:20.102074: Epoch   0 Batch  480/781   test_loss = 1.052\n",
      "2018-06-07T16:41:20.241826: Epoch   0 Batch  500/781   test_loss = 0.971\n",
      "2018-06-07T16:41:20.399948: Epoch   0 Batch  520/781   test_loss = 1.128\n",
      "2018-06-07T16:41:20.540446: Epoch   0 Batch  540/781   test_loss = 0.976\n",
      "2018-06-07T16:41:20.690896: Epoch   0 Batch  560/781   test_loss = 1.214\n",
      "2018-06-07T16:41:20.830944: Epoch   0 Batch  580/781   test_loss = 1.136\n",
      "2018-06-07T16:41:20.987441: Epoch   0 Batch  600/781   test_loss = 1.176\n",
      "2018-06-07T16:41:21.130689: Epoch   0 Batch  620/781   test_loss = 1.180\n",
      "2018-06-07T16:41:21.279579: Epoch   0 Batch  640/781   test_loss = 1.155\n",
      "2018-06-07T16:41:21.423380: Epoch   0 Batch  660/781   test_loss = 1.090\n",
      "2018-06-07T16:41:21.580760: Epoch   0 Batch  680/781   test_loss = 1.362\n",
      "2018-06-07T16:41:21.725819: Epoch   0 Batch  700/781   test_loss = 1.107\n",
      "2018-06-07T16:41:21.875976: Epoch   0 Batch  720/781   test_loss = 1.262\n",
      "2018-06-07T16:41:22.030734: Epoch   0 Batch  740/781   test_loss = 1.220\n",
      "2018-06-07T16:41:22.190786: Epoch   0 Batch  760/781   test_loss = 1.131\n",
      "2018-06-07T16:41:22.335735: Epoch   0 Batch  780/781   test_loss = 1.092\n",
      "2018-06-07T16:41:23.523462: Epoch   1 Batch   15/3125   train_loss = 1.197\n",
      "2018-06-07T16:41:24.198025: Epoch   1 Batch   35/3125   train_loss = 1.110\n",
      "2018-06-07T16:41:24.865544: Epoch   1 Batch   55/3125   train_loss = 1.225\n",
      "2018-06-07T16:41:25.518749: Epoch   1 Batch   75/3125   train_loss = 1.082\n",
      "2018-06-07T16:41:26.198612: Epoch   1 Batch   95/3125   train_loss = 1.002\n",
      "2018-06-07T16:41:26.860957: Epoch   1 Batch  115/3125   train_loss = 1.153\n",
      "2018-06-07T16:41:27.514625: Epoch   1 Batch  135/3125   train_loss = 1.035\n",
      "2018-06-07T16:41:28.178283: Epoch   1 Batch  155/3125   train_loss = 1.121\n",
      "2018-06-07T16:41:28.847770: Epoch   1 Batch  175/3125   train_loss = 1.025\n",
      "2018-06-07T16:41:29.517466: Epoch   1 Batch  195/3125   train_loss = 1.208\n",
      "2018-06-07T16:41:30.228623: Epoch   1 Batch  215/3125   train_loss = 1.095\n",
      "2018-06-07T16:41:30.920214: Epoch   1 Batch  235/3125   train_loss = 1.009\n",
      "2018-06-07T16:41:31.681860: Epoch   1 Batch  255/3125   train_loss = 1.137\n",
      "2018-06-07T16:41:32.517189: Epoch   1 Batch  275/3125   train_loss = 1.012\n",
      "2018-06-07T16:41:33.239368: Epoch   1 Batch  295/3125   train_loss = 0.999\n",
      "2018-06-07T16:41:33.945998: Epoch   1 Batch  315/3125   train_loss = 1.100\n",
      "2018-06-07T16:41:34.709934: Epoch   1 Batch  335/3125   train_loss = 0.945\n",
      "2018-06-07T16:41:35.446473: Epoch   1 Batch  355/3125   train_loss = 1.136\n",
      "2018-06-07T16:41:36.133817: Epoch   1 Batch  375/3125   train_loss = 1.178\n",
      "2018-06-07T16:41:36.877917: Epoch   1 Batch  395/3125   train_loss = 1.026\n",
      "2018-06-07T16:41:37.653272: Epoch   1 Batch  415/3125   train_loss = 1.196\n",
      "2018-06-07T16:41:38.494856: Epoch   1 Batch  435/3125   train_loss = 1.102\n",
      "2018-06-07T16:41:39.298373: Epoch   1 Batch  455/3125   train_loss = 1.115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-07T16:41:40.089638: Epoch   1 Batch  475/3125   train_loss = 1.222\n",
      "2018-06-07T16:41:40.910475: Epoch   1 Batch  495/3125   train_loss = 1.050\n",
      "2018-06-07T16:41:41.612805: Epoch   1 Batch  515/3125   train_loss = 1.133\n",
      "2018-06-07T16:41:42.322016: Epoch   1 Batch  535/3125   train_loss = 1.173\n",
      "2018-06-07T16:41:43.010573: Epoch   1 Batch  555/3125   train_loss = 1.237\n",
      "2018-06-07T16:41:43.697749: Epoch   1 Batch  575/3125   train_loss = 1.077\n",
      "2018-06-07T16:41:44.534857: Epoch   1 Batch  595/3125   train_loss = 1.315\n",
      "2018-06-07T16:41:45.204902: Epoch   1 Batch  615/3125   train_loss = 1.024\n",
      "2018-06-07T16:41:45.866824: Epoch   1 Batch  635/3125   train_loss = 1.137\n",
      "2018-06-07T16:41:46.528390: Epoch   1 Batch  655/3125   train_loss = 0.931\n",
      "2018-06-07T16:41:47.192984: Epoch   1 Batch  675/3125   train_loss = 0.863\n",
      "2018-06-07T16:41:47.981083: Epoch   1 Batch  695/3125   train_loss = 0.981\n",
      "2018-06-07T16:41:48.863102: Epoch   1 Batch  715/3125   train_loss = 1.107\n",
      "2018-06-07T16:41:49.611859: Epoch   1 Batch  735/3125   train_loss = 0.958\n",
      "2018-06-07T16:41:50.373676: Epoch   1 Batch  755/3125   train_loss = 1.152\n",
      "2018-06-07T16:41:51.312495: Epoch   1 Batch  775/3125   train_loss = 0.949\n",
      "2018-06-07T16:41:52.072782: Epoch   1 Batch  795/3125   train_loss = 1.181\n",
      "2018-06-07T16:41:52.870198: Epoch   1 Batch  815/3125   train_loss = 1.063\n",
      "2018-06-07T16:41:53.535570: Epoch   1 Batch  835/3125   train_loss = 1.107\n",
      "2018-06-07T16:41:54.251405: Epoch   1 Batch  855/3125   train_loss = 1.274\n",
      "2018-06-07T16:41:55.008704: Epoch   1 Batch  875/3125   train_loss = 1.123\n",
      "2018-06-07T16:41:55.737473: Epoch   1 Batch  895/3125   train_loss = 1.082\n",
      "2018-06-07T16:41:56.424913: Epoch   1 Batch  915/3125   train_loss = 1.159\n",
      "2018-06-07T16:41:57.125187: Epoch   1 Batch  935/3125   train_loss = 1.190\n",
      "2018-06-07T16:41:57.831101: Epoch   1 Batch  955/3125   train_loss = 1.094\n",
      "2018-06-07T16:41:58.848667: Epoch   1 Batch  975/3125   train_loss = 1.103\n",
      "2018-06-07T16:41:59.571369: Epoch   1 Batch  995/3125   train_loss = 0.936\n",
      "2018-06-07T16:42:00.288226: Epoch   1 Batch 1015/3125   train_loss = 1.079\n",
      "2018-06-07T16:42:01.028773: Epoch   1 Batch 1035/3125   train_loss = 1.048\n",
      "2018-06-07T16:42:01.709580: Epoch   1 Batch 1055/3125   train_loss = 1.119\n",
      "2018-06-07T16:42:02.464578: Epoch   1 Batch 1075/3125   train_loss = 1.025\n",
      "2018-06-07T16:42:03.116261: Epoch   1 Batch 1095/3125   train_loss = 0.910\n",
      "2018-06-07T16:42:03.803742: Epoch   1 Batch 1115/3125   train_loss = 1.130\n",
      "2018-06-07T16:42:04.456559: Epoch   1 Batch 1135/3125   train_loss = 1.013\n",
      "2018-06-07T16:42:05.144666: Epoch   1 Batch 1155/3125   train_loss = 1.071\n",
      "2018-06-07T16:42:05.938770: Epoch   1 Batch 1175/3125   train_loss = 1.136\n",
      "2018-06-07T16:42:06.872119: Epoch   1 Batch 1195/3125   train_loss = 1.228\n",
      "2018-06-07T16:42:07.598323: Epoch   1 Batch 1215/3125   train_loss = 0.882\n",
      "2018-06-07T16:42:08.397056: Epoch   1 Batch 1235/3125   train_loss = 1.056\n",
      "2018-06-07T16:42:09.108166: Epoch   1 Batch 1255/3125   train_loss = 0.928\n",
      "2018-06-07T16:42:09.797462: Epoch   1 Batch 1275/3125   train_loss = 0.986\n",
      "2018-06-07T16:42:10.485981: Epoch   1 Batch 1295/3125   train_loss = 1.003\n",
      "2018-06-07T16:42:11.187198: Epoch   1 Batch 1315/3125   train_loss = 1.108\n",
      "2018-06-07T16:42:11.905646: Epoch   1 Batch 1335/3125   train_loss = 1.019\n",
      "2018-06-07T16:42:12.605739: Epoch   1 Batch 1355/3125   train_loss = 1.026\n",
      "2018-06-07T16:42:13.300561: Epoch   1 Batch 1375/3125   train_loss = 1.117\n",
      "2018-06-07T16:42:14.015880: Epoch   1 Batch 1395/3125   train_loss = 1.011\n",
      "2018-06-07T16:42:14.735788: Epoch   1 Batch 1415/3125   train_loss = 1.013\n",
      "2018-06-07T16:42:15.421467: Epoch   1 Batch 1435/3125   train_loss = 1.187\n",
      "2018-06-07T16:42:16.283690: Epoch   1 Batch 1455/3125   train_loss = 1.190\n",
      "2018-06-07T16:42:16.976921: Epoch   1 Batch 1475/3125   train_loss = 1.037\n",
      "2018-06-07T16:42:17.651942: Epoch   1 Batch 1495/3125   train_loss = 0.968\n",
      "2018-06-07T16:42:18.363867: Epoch   1 Batch 1515/3125   train_loss = 0.933\n",
      "2018-06-07T16:42:19.038111: Epoch   1 Batch 1535/3125   train_loss = 0.907\n",
      "2018-06-07T16:42:19.715145: Epoch   1 Batch 1555/3125   train_loss = 1.096\n",
      "2018-06-07T16:42:20.392176: Epoch   1 Batch 1575/3125   train_loss = 1.002\n",
      "2018-06-07T16:42:21.075114: Epoch   1 Batch 1595/3125   train_loss = 1.051\n",
      "2018-06-07T16:42:21.739669: Epoch   1 Batch 1615/3125   train_loss = 1.028\n",
      "2018-06-07T16:42:22.421338: Epoch   1 Batch 1635/3125   train_loss = 1.106\n",
      "2018-06-07T16:42:23.118180: Epoch   1 Batch 1655/3125   train_loss = 1.083\n",
      "2018-06-07T16:42:23.770623: Epoch   1 Batch 1675/3125   train_loss = 0.964\n",
      "2018-06-07T16:42:24.451606: Epoch   1 Batch 1695/3125   train_loss = 1.041\n",
      "2018-06-07T16:42:25.098344: Epoch   1 Batch 1715/3125   train_loss = 1.006\n",
      "2018-06-07T16:42:25.784597: Epoch   1 Batch 1735/3125   train_loss = 1.165\n",
      "2018-06-07T16:42:26.456967: Epoch   1 Batch 1755/3125   train_loss = 0.967\n",
      "2018-06-07T16:42:27.196469: Epoch   1 Batch 1775/3125   train_loss = 1.030\n",
      "2018-06-07T16:42:27.955089: Epoch   1 Batch 1795/3125   train_loss = 1.070\n",
      "2018-06-07T16:42:28.644989: Epoch   1 Batch 1815/3125   train_loss = 0.985\n",
      "2018-06-07T16:42:29.337067: Epoch   1 Batch 1835/3125   train_loss = 1.104\n",
      "2018-06-07T16:42:30.077973: Epoch   1 Batch 1855/3125   train_loss = 0.974\n",
      "2018-06-07T16:42:30.840804: Epoch   1 Batch 1875/3125   train_loss = 1.090\n",
      "2018-06-07T16:42:31.566946: Epoch   1 Batch 1895/3125   train_loss = 1.004\n",
      "2018-06-07T16:42:32.282213: Epoch   1 Batch 1915/3125   train_loss = 0.900\n",
      "2018-06-07T16:42:33.061089: Epoch   1 Batch 1935/3125   train_loss = 1.019\n",
      "2018-06-07T16:42:33.731450: Epoch   1 Batch 1955/3125   train_loss = 0.988\n",
      "2018-06-07T16:42:34.503896: Epoch   1 Batch 1975/3125   train_loss = 0.982\n",
      "2018-06-07T16:42:35.237059: Epoch   1 Batch 1995/3125   train_loss = 1.169\n",
      "2018-06-07T16:42:35.957509: Epoch   1 Batch 2015/3125   train_loss = 1.125\n",
      "2018-06-07T16:42:36.633165: Epoch   1 Batch 2035/3125   train_loss = 1.116\n",
      "2018-06-07T16:42:37.276376: Epoch   1 Batch 2055/3125   train_loss = 0.961\n",
      "2018-06-07T16:42:37.926071: Epoch   1 Batch 2075/3125   train_loss = 1.144\n",
      "2018-06-07T16:42:38.576632: Epoch   1 Batch 2095/3125   train_loss = 0.975\n",
      "2018-06-07T16:42:39.237431: Epoch   1 Batch 2115/3125   train_loss = 1.162\n",
      "2018-06-07T16:42:39.882916: Epoch   1 Batch 2135/3125   train_loss = 1.015\n",
      "2018-06-07T16:42:40.529360: Epoch   1 Batch 2155/3125   train_loss = 0.984\n",
      "2018-06-07T16:42:41.190548: Epoch   1 Batch 2175/3125   train_loss = 1.070\n",
      "2018-06-07T16:42:41.833993: Epoch   1 Batch 2195/3125   train_loss = 0.989\n",
      "2018-06-07T16:42:42.507817: Epoch   1 Batch 2215/3125   train_loss = 1.012\n",
      "2018-06-07T16:42:43.159029: Epoch   1 Batch 2235/3125   train_loss = 1.118\n",
      "2018-06-07T16:42:43.801291: Epoch   1 Batch 2255/3125   train_loss = 1.160\n",
      "2018-06-07T16:42:44.452853: Epoch   1 Batch 2275/3125   train_loss = 0.870\n",
      "2018-06-07T16:42:45.099596: Epoch   1 Batch 2295/3125   train_loss = 1.246\n",
      "2018-06-07T16:42:45.860055: Epoch   1 Batch 2315/3125   train_loss = 1.099\n",
      "2018-06-07T16:42:46.569868: Epoch   1 Batch 2335/3125   train_loss = 1.008\n",
      "2018-06-07T16:42:47.258000: Epoch   1 Batch 2355/3125   train_loss = 1.101\n",
      "2018-06-07T16:42:47.936354: Epoch   1 Batch 2375/3125   train_loss = 1.188\n",
      "2018-06-07T16:42:48.624012: Epoch   1 Batch 2395/3125   train_loss = 1.026\n",
      "2018-06-07T16:42:49.295198: Epoch   1 Batch 2415/3125   train_loss = 1.013\n",
      "2018-06-07T16:42:50.033083: Epoch   1 Batch 2435/3125   train_loss = 0.931\n",
      "2018-06-07T16:42:50.691221: Epoch   1 Batch 2455/3125   train_loss = 1.076\n",
      "2018-06-07T16:42:51.398796: Epoch   1 Batch 2475/3125   train_loss = 0.994\n",
      "2018-06-07T16:42:52.249624: Epoch   1 Batch 2495/3125   train_loss = 0.981\n",
      "2018-06-07T16:42:52.991684: Epoch   1 Batch 2515/3125   train_loss = 1.037\n",
      "2018-06-07T16:42:53.683064: Epoch   1 Batch 2535/3125   train_loss = 1.099\n",
      "2018-06-07T16:42:54.449194: Epoch   1 Batch 2555/3125   train_loss = 0.938\n",
      "2018-06-07T16:42:55.219849: Epoch   1 Batch 2575/3125   train_loss = 0.938\n",
      "2018-06-07T16:42:56.038596: Epoch   1 Batch 2595/3125   train_loss = 0.956\n",
      "2018-06-07T16:42:56.830143: Epoch   1 Batch 2615/3125   train_loss = 1.090\n",
      "2018-06-07T16:42:57.626849: Epoch   1 Batch 2635/3125   train_loss = 0.941\n",
      "2018-06-07T16:42:58.429774: Epoch   1 Batch 2655/3125   train_loss = 0.970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-07T16:42:59.231976: Epoch   1 Batch 2675/3125   train_loss = 0.997\n",
      "2018-06-07T16:42:59.985142: Epoch   1 Batch 2695/3125   train_loss = 1.104\n",
      "2018-06-07T16:43:00.829722: Epoch   1 Batch 2715/3125   train_loss = 1.002\n",
      "2018-06-07T16:43:01.762550: Epoch   1 Batch 2735/3125   train_loss = 0.875\n",
      "2018-06-07T16:43:02.543941: Epoch   1 Batch 2755/3125   train_loss = 1.127\n",
      "2018-06-07T16:43:03.438141: Epoch   1 Batch 2775/3125   train_loss = 1.056\n",
      "2018-06-07T16:43:04.217210: Epoch   1 Batch 2795/3125   train_loss = 1.023\n",
      "2018-06-07T16:43:04.977952: Epoch   1 Batch 2815/3125   train_loss = 0.953\n",
      "2018-06-07T16:43:05.786872: Epoch   1 Batch 2835/3125   train_loss = 1.057\n",
      "2018-06-07T16:43:06.580819: Epoch   1 Batch 2855/3125   train_loss = 1.059\n",
      "2018-06-07T16:43:07.304784: Epoch   1 Batch 2875/3125   train_loss = 0.999\n",
      "2018-06-07T16:43:08.082768: Epoch   1 Batch 2895/3125   train_loss = 1.069\n",
      "2018-06-07T16:43:08.793148: Epoch   1 Batch 2915/3125   train_loss = 0.971\n",
      "2018-06-07T16:43:09.492085: Epoch   1 Batch 2935/3125   train_loss = 1.077\n",
      "2018-06-07T16:43:10.148207: Epoch   1 Batch 2955/3125   train_loss = 1.117\n",
      "2018-06-07T16:43:10.814909: Epoch   1 Batch 2975/3125   train_loss = 1.034\n",
      "2018-06-07T16:43:11.470750: Epoch   1 Batch 2995/3125   train_loss = 0.871\n",
      "2018-06-07T16:43:12.117485: Epoch   1 Batch 3015/3125   train_loss = 0.970\n",
      "2018-06-07T16:43:12.773537: Epoch   1 Batch 3035/3125   train_loss = 1.017\n",
      "2018-06-07T16:43:13.464991: Epoch   1 Batch 3055/3125   train_loss = 1.068\n",
      "2018-06-07T16:43:14.188526: Epoch   1 Batch 3075/3125   train_loss = 1.025\n",
      "2018-06-07T16:43:14.876656: Epoch   1 Batch 3095/3125   train_loss = 1.004\n",
      "2018-06-07T16:43:15.578244: Epoch   1 Batch 3115/3125   train_loss = 0.876\n",
      "2018-06-07T16:43:16.051087: Epoch   1 Batch   19/781   test_loss = 1.004\n",
      "2018-06-07T16:43:16.207101: Epoch   1 Batch   39/781   test_loss = 0.841\n",
      "2018-06-07T16:43:16.369243: Epoch   1 Batch   59/781   test_loss = 0.889\n",
      "2018-06-07T16:43:16.525179: Epoch   1 Batch   79/781   test_loss = 0.986\n",
      "2018-06-07T16:43:16.704846: Epoch   1 Batch   99/781   test_loss = 1.027\n",
      "2018-06-07T16:43:16.870998: Epoch   1 Batch  119/781   test_loss = 0.899\n",
      "2018-06-07T16:43:17.023243: Epoch   1 Batch  139/781   test_loss = 1.033\n",
      "2018-06-07T16:43:17.296456: Epoch   1 Batch  159/781   test_loss = 1.031\n",
      "2018-06-07T16:43:17.490221: Epoch   1 Batch  179/781   test_loss = 0.915\n",
      "2018-06-07T16:43:17.646014: Epoch   1 Batch  199/781   test_loss = 0.881\n",
      "2018-06-07T16:43:17.807574: Epoch   1 Batch  219/781   test_loss = 1.041\n",
      "2018-06-07T16:43:17.963975: Epoch   1 Batch  239/781   test_loss = 1.158\n",
      "2018-06-07T16:43:18.113062: Epoch   1 Batch  259/781   test_loss = 0.998\n",
      "2018-06-07T16:43:18.273375: Epoch   1 Batch  279/781   test_loss = 1.118\n",
      "2018-06-07T16:43:18.434114: Epoch   1 Batch  299/781   test_loss = 1.180\n",
      "2018-06-07T16:43:18.593167: Epoch   1 Batch  319/781   test_loss = 0.975\n",
      "2018-06-07T16:43:18.749204: Epoch   1 Batch  339/781   test_loss = 0.969\n",
      "2018-06-07T16:43:18.911280: Epoch   1 Batch  359/781   test_loss = 0.916\n",
      "2018-06-07T16:43:19.071786: Epoch   1 Batch  379/781   test_loss = 0.975\n",
      "2018-06-07T16:43:19.234043: Epoch   1 Batch  399/781   test_loss = 0.874\n",
      "2018-06-07T16:43:19.385142: Epoch   1 Batch  419/781   test_loss = 0.972\n",
      "2018-06-07T16:43:19.543067: Epoch   1 Batch  439/781   test_loss = 0.994\n",
      "2018-06-07T16:43:19.698514: Epoch   1 Batch  459/781   test_loss = 1.023\n",
      "2018-06-07T16:43:19.858103: Epoch   1 Batch  479/781   test_loss = 1.019\n",
      "2018-06-07T16:43:20.016597: Epoch   1 Batch  499/781   test_loss = 0.999\n",
      "2018-06-07T16:43:20.227266: Epoch   1 Batch  519/781   test_loss = 1.092\n",
      "2018-06-07T16:43:20.419420: Epoch   1 Batch  539/781   test_loss = 0.875\n",
      "2018-06-07T16:43:20.584379: Epoch   1 Batch  559/781   test_loss = 1.133\n",
      "2018-06-07T16:43:20.748774: Epoch   1 Batch  579/781   test_loss = 1.044\n",
      "2018-06-07T16:43:20.912874: Epoch   1 Batch  599/781   test_loss = 0.947\n",
      "2018-06-07T16:43:21.074651: Epoch   1 Batch  619/781   test_loss = 1.138\n",
      "2018-06-07T16:43:21.245964: Epoch   1 Batch  639/781   test_loss = 0.925\n",
      "2018-06-07T16:43:21.407550: Epoch   1 Batch  659/781   test_loss = 1.155\n",
      "2018-06-07T16:43:21.573654: Epoch   1 Batch  679/781   test_loss = 1.162\n",
      "2018-06-07T16:43:21.733864: Epoch   1 Batch  699/781   test_loss = 0.828\n",
      "2018-06-07T16:43:21.904753: Epoch   1 Batch  719/781   test_loss = 0.934\n",
      "2018-06-07T16:43:22.069123: Epoch   1 Batch  739/781   test_loss = 0.937\n",
      "2018-06-07T16:43:22.237852: Epoch   1 Batch  759/781   test_loss = 0.914\n",
      "2018-06-07T16:43:22.397706: Epoch   1 Batch  779/781   test_loss = 0.782\n",
      "2018-06-07T16:43:23.479771: Epoch   2 Batch   10/3125   train_loss = 0.899\n",
      "2018-06-07T16:43:24.189513: Epoch   2 Batch   30/3125   train_loss = 1.001\n",
      "2018-06-07T16:43:24.939649: Epoch   2 Batch   50/3125   train_loss = 1.025\n",
      "2018-06-07T16:43:25.792688: Epoch   2 Batch   70/3125   train_loss = 1.033\n",
      "2018-06-07T16:43:26.567593: Epoch   2 Batch   90/3125   train_loss = 0.987\n",
      "2018-06-07T16:43:27.354125: Epoch   2 Batch  110/3125   train_loss = 0.912\n",
      "2018-06-07T16:43:28.046547: Epoch   2 Batch  130/3125   train_loss = 0.940\n",
      "2018-06-07T16:43:28.753494: Epoch   2 Batch  150/3125   train_loss = 1.136\n",
      "2018-06-07T16:43:29.477052: Epoch   2 Batch  170/3125   train_loss = 0.960\n",
      "2018-06-07T16:43:30.316749: Epoch   2 Batch  190/3125   train_loss = 0.990\n",
      "2018-06-07T16:43:31.327877: Epoch   2 Batch  210/3125   train_loss = 0.915\n",
      "2018-06-07T16:43:32.023131: Epoch   2 Batch  230/3125   train_loss = 1.041\n",
      "2018-06-07T16:43:32.734850: Epoch   2 Batch  250/3125   train_loss = 0.955\n",
      "2018-06-07T16:43:33.416747: Epoch   2 Batch  270/3125   train_loss = 0.769\n",
      "2018-06-07T16:43:34.111967: Epoch   2 Batch  290/3125   train_loss = 1.027\n",
      "2018-06-07T16:43:34.887722: Epoch   2 Batch  310/3125   train_loss = 0.970\n",
      "2018-06-07T16:43:35.564174: Epoch   2 Batch  330/3125   train_loss = 1.027\n",
      "2018-06-07T16:43:36.257465: Epoch   2 Batch  350/3125   train_loss = 0.931\n",
      "2018-06-07T16:43:37.020299: Epoch   2 Batch  370/3125   train_loss = 1.102\n",
      "2018-06-07T16:43:37.735475: Epoch   2 Batch  390/3125   train_loss = 1.152\n",
      "2018-06-07T16:43:38.472837: Epoch   2 Batch  410/3125   train_loss = 0.918\n",
      "2018-06-07T16:43:39.210056: Epoch   2 Batch  430/3125   train_loss = 1.160\n",
      "2018-06-07T16:43:39.886298: Epoch   2 Batch  450/3125   train_loss = 0.967\n",
      "2018-06-07T16:43:40.541677: Epoch   2 Batch  470/3125   train_loss = 0.952\n",
      "2018-06-07T16:43:41.268301: Epoch   2 Batch  490/3125   train_loss = 1.041\n",
      "2018-06-07T16:43:41.944746: Epoch   2 Batch  510/3125   train_loss = 1.038\n",
      "2018-06-07T16:43:42.736678: Epoch   2 Batch  530/3125   train_loss = 0.919\n",
      "2018-06-07T16:43:43.480287: Epoch   2 Batch  550/3125   train_loss = 0.929\n",
      "2018-06-07T16:43:44.279357: Epoch   2 Batch  570/3125   train_loss = 1.058\n",
      "2018-06-07T16:43:45.004753: Epoch   2 Batch  590/3125   train_loss = 1.007\n",
      "2018-06-07T16:43:45.728297: Epoch   2 Batch  610/3125   train_loss = 0.975\n",
      "2018-06-07T16:43:46.452521: Epoch   2 Batch  630/3125   train_loss = 1.034\n",
      "2018-06-07T16:43:47.127022: Epoch   2 Batch  650/3125   train_loss = 1.048\n",
      "2018-06-07T16:43:47.835865: Epoch   2 Batch  670/3125   train_loss = 0.921\n",
      "2018-06-07T16:43:48.613005: Epoch   2 Batch  690/3125   train_loss = 0.952\n",
      "2018-06-07T16:43:49.435829: Epoch   2 Batch  710/3125   train_loss = 0.944\n",
      "2018-06-07T16:43:50.226655: Epoch   2 Batch  730/3125   train_loss = 0.840\n",
      "2018-06-07T16:43:50.877408: Epoch   2 Batch  750/3125   train_loss = 0.956\n",
      "2018-06-07T16:43:51.576077: Epoch   2 Batch  770/3125   train_loss = 0.886\n",
      "2018-06-07T16:43:52.432377: Epoch   2 Batch  790/3125   train_loss = 0.892\n",
      "2018-06-07T16:43:53.181594: Epoch   2 Batch  810/3125   train_loss = 0.839\n",
      "2018-06-07T16:43:53.898170: Epoch   2 Batch  830/3125   train_loss = 0.829\n",
      "2018-06-07T16:43:54.695536: Epoch   2 Batch  850/3125   train_loss = 0.994\n",
      "2018-06-07T16:43:55.385441: Epoch   2 Batch  870/3125   train_loss = 0.887\n",
      "2018-06-07T16:43:56.024195: Epoch   2 Batch  890/3125   train_loss = 0.898\n",
      "2018-06-07T16:43:56.701618: Epoch   2 Batch  910/3125   train_loss = 0.979\n",
      "2018-06-07T16:43:57.437456: Epoch   2 Batch  930/3125   train_loss = 0.957\n",
      "2018-06-07T16:43:58.154380: Epoch   2 Batch  950/3125   train_loss = 0.905\n",
      "2018-06-07T16:43:58.876517: Epoch   2 Batch  970/3125   train_loss = 1.045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-07T16:43:59.685350: Epoch   2 Batch  990/3125   train_loss = 0.883\n",
      "2018-06-07T16:44:00.417115: Epoch   2 Batch 1010/3125   train_loss = 1.119\n",
      "2018-06-07T16:44:01.202035: Epoch   2 Batch 1030/3125   train_loss = 0.874\n",
      "2018-06-07T16:44:01.972438: Epoch   2 Batch 1050/3125   train_loss = 0.950\n",
      "2018-06-07T16:44:02.667024: Epoch   2 Batch 1070/3125   train_loss = 0.894\n",
      "2018-06-07T16:44:03.352108: Epoch   2 Batch 1090/3125   train_loss = 1.037\n",
      "2018-06-07T16:44:04.081473: Epoch   2 Batch 1110/3125   train_loss = 1.094\n",
      "2018-06-07T16:44:04.840324: Epoch   2 Batch 1130/3125   train_loss = 0.930\n",
      "2018-06-07T16:44:05.522834: Epoch   2 Batch 1150/3125   train_loss = 0.984\n",
      "2018-06-07T16:44:06.167351: Epoch   2 Batch 1170/3125   train_loss = 1.028\n",
      "2018-06-07T16:44:06.810466: Epoch   2 Batch 1190/3125   train_loss = 1.000\n",
      "2018-06-07T16:44:07.474430: Epoch   2 Batch 1210/3125   train_loss = 0.848\n",
      "2018-06-07T16:44:08.152299: Epoch   2 Batch 1230/3125   train_loss = 0.852\n",
      "2018-06-07T16:44:08.792590: Epoch   2 Batch 1250/3125   train_loss = 0.960\n",
      "2018-06-07T16:44:09.435631: Epoch   2 Batch 1270/3125   train_loss = 0.999\n",
      "2018-06-07T16:44:10.100077: Epoch   2 Batch 1290/3125   train_loss = 0.906\n",
      "2018-06-07T16:44:10.778970: Epoch   2 Batch 1310/3125   train_loss = 0.971\n",
      "2018-06-07T16:44:11.450727: Epoch   2 Batch 1330/3125   train_loss = 1.099\n",
      "2018-06-07T16:44:12.118047: Epoch   2 Batch 1350/3125   train_loss = 0.939\n",
      "2018-06-07T16:44:12.766515: Epoch   2 Batch 1370/3125   train_loss = 0.859\n",
      "2018-06-07T16:44:13.431895: Epoch   2 Batch 1390/3125   train_loss = 0.971\n",
      "2018-06-07T16:44:14.154292: Epoch   2 Batch 1410/3125   train_loss = 0.960\n",
      "2018-06-07T16:44:14.814842: Epoch   2 Batch 1430/3125   train_loss = 1.003\n",
      "2018-06-07T16:44:15.459536: Epoch   2 Batch 1450/3125   train_loss = 0.978\n",
      "2018-06-07T16:44:16.111754: Epoch   2 Batch 1470/3125   train_loss = 0.999\n",
      "2018-06-07T16:44:16.752666: Epoch   2 Batch 1490/3125   train_loss = 1.030\n",
      "2018-06-07T16:44:17.398388: Epoch   2 Batch 1510/3125   train_loss = 1.026\n",
      "2018-06-07T16:44:18.041755: Epoch   2 Batch 1530/3125   train_loss = 1.101\n",
      "2018-06-07T16:44:18.716006: Epoch   2 Batch 1550/3125   train_loss = 0.850\n",
      "2018-06-07T16:44:19.369889: Epoch   2 Batch 1570/3125   train_loss = 0.989\n",
      "2018-06-07T16:44:20.009795: Epoch   2 Batch 1590/3125   train_loss = 0.951\n",
      "2018-06-07T16:44:20.824216: Epoch   2 Batch 1610/3125   train_loss = 0.994\n",
      "2018-06-07T16:44:21.574960: Epoch   2 Batch 1630/3125   train_loss = 1.065\n",
      "2018-06-07T16:44:22.310153: Epoch   2 Batch 1650/3125   train_loss = 0.845\n",
      "2018-06-07T16:44:23.148856: Epoch   2 Batch 1670/3125   train_loss = 0.797\n",
      "2018-06-07T16:44:23.989317: Epoch   2 Batch 1690/3125   train_loss = 0.963\n",
      "2018-06-07T16:44:24.686982: Epoch   2 Batch 1710/3125   train_loss = 0.963\n",
      "2018-06-07T16:44:25.410627: Epoch   2 Batch 1730/3125   train_loss = 0.945\n",
      "2018-06-07T16:44:26.211228: Epoch   2 Batch 1750/3125   train_loss = 0.791\n",
      "2018-06-07T16:44:26.923048: Epoch   2 Batch 1770/3125   train_loss = 1.149\n",
      "2018-06-07T16:44:27.743144: Epoch   2 Batch 1790/3125   train_loss = 1.008\n",
      "2018-06-07T16:44:28.403887: Epoch   2 Batch 1810/3125   train_loss = 0.991\n",
      "2018-06-07T16:44:29.098797: Epoch   2 Batch 1830/3125   train_loss = 1.022\n",
      "2018-06-07T16:44:29.943364: Epoch   2 Batch 1850/3125   train_loss = 0.926\n",
      "2018-06-07T16:44:30.666411: Epoch   2 Batch 1870/3125   train_loss = 1.025\n",
      "2018-06-07T16:44:31.623513: Epoch   2 Batch 1890/3125   train_loss = 0.810\n",
      "2018-06-07T16:44:32.502511: Epoch   2 Batch 1910/3125   train_loss = 0.869\n",
      "2018-06-07T16:44:33.388463: Epoch   2 Batch 1930/3125   train_loss = 1.040\n",
      "2018-06-07T16:44:34.167153: Epoch   2 Batch 1950/3125   train_loss = 0.800\n",
      "2018-06-07T16:44:34.884342: Epoch   2 Batch 1970/3125   train_loss = 0.991\n",
      "2018-06-07T16:44:35.558703: Epoch   2 Batch 1990/3125   train_loss = 0.841\n",
      "2018-06-07T16:44:36.238322: Epoch   2 Batch 2010/3125   train_loss = 0.797\n",
      "2018-06-07T16:44:36.916150: Epoch   2 Batch 2030/3125   train_loss = 0.926\n",
      "2018-06-07T16:44:37.609839: Epoch   2 Batch 2050/3125   train_loss = 0.979\n",
      "2018-06-07T16:44:38.289216: Epoch   2 Batch 2070/3125   train_loss = 0.899\n",
      "2018-06-07T16:44:38.981391: Epoch   2 Batch 2090/3125   train_loss = 0.939\n",
      "2018-06-07T16:44:39.655126: Epoch   2 Batch 2110/3125   train_loss = 1.058\n",
      "2018-06-07T16:44:40.335630: Epoch   2 Batch 2130/3125   train_loss = 0.963\n",
      "2018-06-07T16:44:41.005867: Epoch   2 Batch 2150/3125   train_loss = 0.933\n",
      "2018-06-07T16:44:41.695036: Epoch   2 Batch 2170/3125   train_loss = 0.874\n",
      "2018-06-07T16:44:42.382621: Epoch   2 Batch 2190/3125   train_loss = 0.912\n",
      "2018-06-07T16:44:43.066447: Epoch   2 Batch 2210/3125   train_loss = 1.001\n",
      "2018-06-07T16:44:43.802768: Epoch   2 Batch 2230/3125   train_loss = 0.905\n",
      "2018-06-07T16:44:44.538354: Epoch   2 Batch 2250/3125   train_loss = 1.028\n",
      "2018-06-07T16:44:45.247342: Epoch   2 Batch 2270/3125   train_loss = 0.963\n",
      "2018-06-07T16:44:45.956848: Epoch   2 Batch 2290/3125   train_loss = 0.856\n",
      "2018-06-07T16:44:46.699406: Epoch   2 Batch 2310/3125   train_loss = 0.927\n",
      "2018-06-07T16:44:47.424923: Epoch   2 Batch 2330/3125   train_loss = 1.075\n",
      "2018-06-07T16:44:48.111224: Epoch   2 Batch 2350/3125   train_loss = 1.022\n",
      "2018-06-07T16:44:48.832555: Epoch   2 Batch 2370/3125   train_loss = 0.931\n",
      "2018-06-07T16:44:49.521063: Epoch   2 Batch 2390/3125   train_loss = 0.982\n",
      "2018-06-07T16:44:50.215550: Epoch   2 Batch 2410/3125   train_loss = 1.066\n",
      "2018-06-07T16:44:50.994584: Epoch   2 Batch 2430/3125   train_loss = 0.950\n",
      "2018-06-07T16:44:51.701166: Epoch   2 Batch 2450/3125   train_loss = 0.964\n",
      "2018-06-07T16:44:52.376807: Epoch   2 Batch 2470/3125   train_loss = 1.005\n",
      "2018-06-07T16:44:53.114357: Epoch   2 Batch 2490/3125   train_loss = 1.021\n",
      "2018-06-07T16:44:53.804212: Epoch   2 Batch 2510/3125   train_loss = 1.001\n",
      "2018-06-07T16:44:54.501356: Epoch   2 Batch 2530/3125   train_loss = 0.769\n",
      "2018-06-07T16:44:55.190103: Epoch   2 Batch 2550/3125   train_loss = 1.009\n",
      "2018-06-07T16:44:55.905093: Epoch   2 Batch 2570/3125   train_loss = 0.969\n",
      "2018-06-07T16:44:56.641673: Epoch   2 Batch 2590/3125   train_loss = 0.991\n",
      "2018-06-07T16:44:57.352727: Epoch   2 Batch 2610/3125   train_loss = 1.053\n",
      "2018-06-07T16:44:58.035090: Epoch   2 Batch 2630/3125   train_loss = 0.665\n",
      "2018-06-07T16:44:58.731408: Epoch   2 Batch 2650/3125   train_loss = 0.911\n",
      "2018-06-07T16:44:59.503250: Epoch   2 Batch 2670/3125   train_loss = 0.948\n",
      "2018-06-07T16:45:00.215488: Epoch   2 Batch 2690/3125   train_loss = 0.930\n",
      "2018-06-07T16:45:00.915233: Epoch   2 Batch 2710/3125   train_loss = 0.835\n",
      "2018-06-07T16:45:01.601413: Epoch   2 Batch 2730/3125   train_loss = 1.066\n",
      "2018-06-07T16:45:02.369285: Epoch   2 Batch 2750/3125   train_loss = 1.005\n",
      "2018-06-07T16:45:03.071689: Epoch   2 Batch 2770/3125   train_loss = 0.944\n",
      "2018-06-07T16:45:03.746542: Epoch   2 Batch 2790/3125   train_loss = 0.886\n",
      "2018-06-07T16:45:04.495766: Epoch   2 Batch 2810/3125   train_loss = 0.933\n",
      "2018-06-07T16:45:05.195811: Epoch   2 Batch 2830/3125   train_loss = 0.807\n",
      "2018-06-07T16:45:05.877740: Epoch   2 Batch 2850/3125   train_loss = 0.977\n",
      "2018-06-07T16:45:06.614725: Epoch   2 Batch 2870/3125   train_loss = 0.782\n",
      "2018-06-07T16:45:07.353443: Epoch   2 Batch 2890/3125   train_loss = 0.744\n",
      "2018-06-07T16:45:08.038453: Epoch   2 Batch 2910/3125   train_loss = 0.955\n",
      "2018-06-07T16:45:08.770602: Epoch   2 Batch 2930/3125   train_loss = 0.790\n",
      "2018-06-07T16:45:09.490268: Epoch   2 Batch 2950/3125   train_loss = 1.007\n",
      "2018-06-07T16:45:10.210492: Epoch   2 Batch 2970/3125   train_loss = 0.926\n",
      "2018-06-07T16:45:10.945006: Epoch   2 Batch 2990/3125   train_loss = 0.888\n",
      "2018-06-07T16:45:11.702347: Epoch   2 Batch 3010/3125   train_loss = 0.954\n",
      "2018-06-07T16:45:12.405502: Epoch   2 Batch 3030/3125   train_loss = 0.945\n",
      "2018-06-07T16:45:13.118048: Epoch   2 Batch 3050/3125   train_loss = 0.937\n",
      "2018-06-07T16:45:13.832609: Epoch   2 Batch 3070/3125   train_loss = 0.851\n",
      "2018-06-07T16:45:14.534099: Epoch   2 Batch 3090/3125   train_loss = 0.804\n",
      "2018-06-07T16:45:15.250435: Epoch   2 Batch 3110/3125   train_loss = 0.812\n",
      "2018-06-07T16:45:15.894201: Epoch   2 Batch   18/781   test_loss = 0.860\n",
      "2018-06-07T16:45:16.045419: Epoch   2 Batch   38/781   test_loss = 0.856\n",
      "2018-06-07T16:45:16.213190: Epoch   2 Batch   58/781   test_loss = 0.846\n",
      "2018-06-07T16:45:16.400039: Epoch   2 Batch   78/781   test_loss = 0.893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-07T16:45:16.571108: Epoch   2 Batch   98/781   test_loss = 0.975\n",
      "2018-06-07T16:45:16.747949: Epoch   2 Batch  118/781   test_loss = 0.848\n",
      "2018-06-07T16:45:16.948334: Epoch   2 Batch  138/781   test_loss = 1.016\n",
      "2018-06-07T16:45:17.110878: Epoch   2 Batch  158/781   test_loss = 0.834\n",
      "2018-06-07T16:45:17.277645: Epoch   2 Batch  178/781   test_loss = 0.843\n",
      "2018-06-07T16:45:17.441642: Epoch   2 Batch  198/781   test_loss = 0.909\n",
      "2018-06-07T16:45:17.601258: Epoch   2 Batch  218/781   test_loss = 0.982\n",
      "2018-06-07T16:45:17.758739: Epoch   2 Batch  238/781   test_loss = 0.953\n",
      "2018-06-07T16:45:17.931666: Epoch   2 Batch  258/781   test_loss = 0.995\n",
      "2018-06-07T16:45:18.080999: Epoch   2 Batch  278/781   test_loss = 1.094\n",
      "2018-06-07T16:45:18.247774: Epoch   2 Batch  298/781   test_loss = 0.871\n",
      "2018-06-07T16:45:18.414097: Epoch   2 Batch  318/781   test_loss = 0.896\n",
      "2018-06-07T16:45:18.582796: Epoch   2 Batch  338/781   test_loss = 0.952\n",
      "2018-06-07T16:45:18.739592: Epoch   2 Batch  358/781   test_loss = 0.910\n",
      "2018-06-07T16:45:18.905510: Epoch   2 Batch  378/781   test_loss = 0.887\n",
      "2018-06-07T16:45:19.068385: Epoch   2 Batch  398/781   test_loss = 0.820\n",
      "2018-06-07T16:45:19.227460: Epoch   2 Batch  418/781   test_loss = 0.953\n",
      "2018-06-07T16:45:19.383518: Epoch   2 Batch  438/781   test_loss = 1.016\n",
      "2018-06-07T16:45:19.552333: Epoch   2 Batch  458/781   test_loss = 0.927\n",
      "2018-06-07T16:45:19.708216: Epoch   2 Batch  478/781   test_loss = 0.981\n",
      "2018-06-07T16:45:19.873434: Epoch   2 Batch  498/781   test_loss = 0.771\n",
      "2018-06-07T16:45:20.039146: Epoch   2 Batch  518/781   test_loss = 0.926\n",
      "2018-06-07T16:45:20.201169: Epoch   2 Batch  538/781   test_loss = 0.829\n",
      "2018-06-07T16:45:20.359309: Epoch   2 Batch  558/781   test_loss = 0.886\n",
      "2018-06-07T16:45:20.525370: Epoch   2 Batch  578/781   test_loss = 0.897\n",
      "2018-06-07T16:45:20.682682: Epoch   2 Batch  598/781   test_loss = 1.074\n",
      "2018-06-07T16:45:20.847765: Epoch   2 Batch  618/781   test_loss = 0.836\n",
      "2018-06-07T16:45:21.000449: Epoch   2 Batch  638/781   test_loss = 0.865\n",
      "2018-06-07T16:45:21.172419: Epoch   2 Batch  658/781   test_loss = 1.026\n",
      "2018-06-07T16:45:21.332377: Epoch   2 Batch  678/781   test_loss = 0.986\n",
      "2018-06-07T16:45:21.495685: Epoch   2 Batch  698/781   test_loss = 0.856\n",
      "2018-06-07T16:45:21.663034: Epoch   2 Batch  718/781   test_loss = 1.019\n",
      "2018-06-07T16:45:21.829571: Epoch   2 Batch  738/781   test_loss = 0.819\n",
      "2018-06-07T16:45:21.982045: Epoch   2 Batch  758/781   test_loss = 0.963\n",
      "2018-06-07T16:45:22.155354: Epoch   2 Batch  778/781   test_loss = 0.906\n",
      "2018-06-07T16:45:23.155946: Epoch   3 Batch    5/3125   train_loss = 0.887\n",
      "2018-06-07T16:45:23.870711: Epoch   3 Batch   25/3125   train_loss = 0.952\n",
      "2018-06-07T16:45:24.555570: Epoch   3 Batch   45/3125   train_loss = 0.813\n",
      "2018-06-07T16:45:25.252301: Epoch   3 Batch   65/3125   train_loss = 0.942\n",
      "2018-06-07T16:45:25.969771: Epoch   3 Batch   85/3125   train_loss = 0.795\n",
      "2018-06-07T16:45:26.674064: Epoch   3 Batch  105/3125   train_loss = 0.776\n",
      "2018-06-07T16:45:27.447820: Epoch   3 Batch  125/3125   train_loss = 0.852\n",
      "2018-06-07T16:45:28.190389: Epoch   3 Batch  145/3125   train_loss = 0.869\n",
      "2018-06-07T16:45:28.893170: Epoch   3 Batch  165/3125   train_loss = 0.881\n",
      "2018-06-07T16:45:29.597163: Epoch   3 Batch  185/3125   train_loss = 0.834\n",
      "2018-06-07T16:45:30.367276: Epoch   3 Batch  205/3125   train_loss = 0.810\n",
      "2018-06-07T16:45:31.061967: Epoch   3 Batch  225/3125   train_loss = 0.844\n",
      "2018-06-07T16:45:31.777538: Epoch   3 Batch  245/3125   train_loss = 1.070\n",
      "2018-06-07T16:45:32.494592: Epoch   3 Batch  265/3125   train_loss = 0.924\n",
      "2018-06-07T16:45:33.208966: Epoch   3 Batch  285/3125   train_loss = 0.932\n",
      "2018-06-07T16:45:33.911886: Epoch   3 Batch  305/3125   train_loss = 0.863\n",
      "2018-06-07T16:45:34.646861: Epoch   3 Batch  325/3125   train_loss = 0.908\n",
      "2018-06-07T16:45:35.345042: Epoch   3 Batch  345/3125   train_loss = 0.973\n",
      "2018-06-07T16:45:36.038448: Epoch   3 Batch  365/3125   train_loss = 0.834\n",
      "2018-06-07T16:45:36.729404: Epoch   3 Batch  385/3125   train_loss = 0.902\n",
      "2018-06-07T16:45:37.437110: Epoch   3 Batch  405/3125   train_loss = 0.869\n",
      "2018-06-07T16:45:38.135358: Epoch   3 Batch  425/3125   train_loss = 0.970\n",
      "2018-06-07T16:45:38.832122: Epoch   3 Batch  445/3125   train_loss = 0.961\n",
      "2018-06-07T16:45:39.518413: Epoch   3 Batch  465/3125   train_loss = 0.870\n",
      "2018-06-07T16:45:40.302256: Epoch   3 Batch  485/3125   train_loss = 0.944\n",
      "2018-06-07T16:45:41.000897: Epoch   3 Batch  505/3125   train_loss = 0.851\n",
      "2018-06-07T16:45:41.701841: Epoch   3 Batch  525/3125   train_loss = 0.977\n",
      "2018-06-07T16:45:42.402121: Epoch   3 Batch  545/3125   train_loss = 0.865\n",
      "2018-06-07T16:45:43.142379: Epoch   3 Batch  565/3125   train_loss = 1.065\n",
      "2018-06-07T16:45:43.845825: Epoch   3 Batch  585/3125   train_loss = 0.865\n",
      "2018-06-07T16:45:44.553633: Epoch   3 Batch  605/3125   train_loss = 0.899\n",
      "2018-06-07T16:45:45.249361: Epoch   3 Batch  625/3125   train_loss = 0.909\n",
      "2018-06-07T16:45:45.938978: Epoch   3 Batch  645/3125   train_loss = 0.926\n",
      "2018-06-07T16:45:46.637306: Epoch   3 Batch  665/3125   train_loss = 0.995\n",
      "2018-06-07T16:45:47.365081: Epoch   3 Batch  685/3125   train_loss = 0.908\n",
      "2018-06-07T16:45:48.076635: Epoch   3 Batch  705/3125   train_loss = 1.060\n",
      "2018-06-07T16:45:48.795779: Epoch   3 Batch  725/3125   train_loss = 0.914\n",
      "2018-06-07T16:45:49.579184: Epoch   3 Batch  745/3125   train_loss = 0.848\n",
      "2018-06-07T16:45:50.292176: Epoch   3 Batch  765/3125   train_loss = 0.804\n",
      "2018-06-07T16:45:51.008044: Epoch   3 Batch  785/3125   train_loss = 1.063\n",
      "2018-06-07T16:45:51.737902: Epoch   3 Batch  805/3125   train_loss = 0.870\n",
      "2018-06-07T16:45:52.476026: Epoch   3 Batch  825/3125   train_loss = 0.900\n",
      "2018-06-07T16:45:53.228853: Epoch   3 Batch  845/3125   train_loss = 0.942\n",
      "2018-06-07T16:45:53.911431: Epoch   3 Batch  865/3125   train_loss = 0.975\n",
      "2018-06-07T16:45:54.640419: Epoch   3 Batch  885/3125   train_loss = 0.915\n",
      "2018-06-07T16:45:55.333309: Epoch   3 Batch  905/3125   train_loss = 1.014\n",
      "2018-06-07T16:45:56.035438: Epoch   3 Batch  925/3125   train_loss = 0.925\n",
      "2018-06-07T16:45:56.778210: Epoch   3 Batch  945/3125   train_loss = 0.917\n",
      "2018-06-07T16:45:57.514652: Epoch   3 Batch  965/3125   train_loss = 0.781\n",
      "2018-06-07T16:45:58.191115: Epoch   3 Batch  985/3125   train_loss = 0.947\n",
      "2018-06-07T16:45:58.931503: Epoch   3 Batch 1005/3125   train_loss = 0.817\n",
      "2018-06-07T16:45:59.640969: Epoch   3 Batch 1025/3125   train_loss = 0.928\n",
      "2018-06-07T16:46:00.313143: Epoch   3 Batch 1045/3125   train_loss = 1.131\n",
      "2018-06-07T16:46:01.006714: Epoch   3 Batch 1065/3125   train_loss = 0.861\n",
      "2018-06-07T16:46:01.686443: Epoch   3 Batch 1085/3125   train_loss = 0.768\n",
      "2018-06-07T16:46:02.412242: Epoch   3 Batch 1105/3125   train_loss = 0.861\n",
      "2018-06-07T16:46:03.164858: Epoch   3 Batch 1125/3125   train_loss = 0.870\n",
      "2018-06-07T16:46:03.840962: Epoch   3 Batch 1145/3125   train_loss = 0.910\n",
      "2018-06-07T16:46:04.542526: Epoch   3 Batch 1165/3125   train_loss = 1.022\n",
      "2018-06-07T16:46:05.287269: Epoch   3 Batch 1185/3125   train_loss = 0.869\n",
      "2018-06-07T16:46:05.982315: Epoch   3 Batch 1205/3125   train_loss = 0.819\n",
      "2018-06-07T16:46:06.681203: Epoch   3 Batch 1225/3125   train_loss = 0.919\n",
      "2018-06-07T16:46:07.367041: Epoch   3 Batch 1245/3125   train_loss = 1.030\n",
      "2018-06-07T16:46:08.049992: Epoch   3 Batch 1265/3125   train_loss = 0.884\n",
      "2018-06-07T16:46:08.724736: Epoch   3 Batch 1285/3125   train_loss = 0.979\n",
      "2018-06-07T16:46:09.405026: Epoch   3 Batch 1305/3125   train_loss = 0.808\n",
      "2018-06-07T16:46:10.120722: Epoch   3 Batch 1325/3125   train_loss = 0.905\n",
      "2018-06-07T16:46:10.844342: Epoch   3 Batch 1345/3125   train_loss = 0.945\n",
      "2018-06-07T16:46:11.581975: Epoch   3 Batch 1365/3125   train_loss = 0.803\n",
      "2018-06-07T16:46:12.270954: Epoch   3 Batch 1385/3125   train_loss = 0.815\n",
      "2018-06-07T16:46:12.980384: Epoch   3 Batch 1405/3125   train_loss = 0.868\n",
      "2018-06-07T16:46:13.698606: Epoch   3 Batch 1425/3125   train_loss = 1.016\n",
      "2018-06-07T16:46:14.395091: Epoch   3 Batch 1445/3125   train_loss = 1.019\n",
      "2018-06-07T16:46:15.078180: Epoch   3 Batch 1465/3125   train_loss = 0.909\n",
      "2018-06-07T16:46:15.763761: Epoch   3 Batch 1485/3125   train_loss = 0.957\n",
      "2018-06-07T16:46:16.488698: Epoch   3 Batch 1505/3125   train_loss = 0.786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-07T16:46:17.148482: Epoch   3 Batch 1525/3125   train_loss = 0.770\n",
      "2018-06-07T16:46:17.840121: Epoch   3 Batch 1545/3125   train_loss = 0.892\n",
      "2018-06-07T16:46:18.539661: Epoch   3 Batch 1565/3125   train_loss = 0.994\n",
      "2018-06-07T16:46:19.209412: Epoch   3 Batch 1585/3125   train_loss = 0.846\n",
      "2018-06-07T16:46:19.893889: Epoch   3 Batch 1605/3125   train_loss = 0.857\n",
      "2018-06-07T16:46:20.587350: Epoch   3 Batch 1625/3125   train_loss = 0.930\n",
      "2018-06-07T16:46:21.264848: Epoch   3 Batch 1645/3125   train_loss = 0.963\n",
      "2018-06-07T16:46:21.953882: Epoch   3 Batch 1665/3125   train_loss = 0.901\n",
      "2018-06-07T16:46:22.646910: Epoch   3 Batch 1685/3125   train_loss = 1.025\n",
      "2018-06-07T16:46:23.388191: Epoch   3 Batch 1705/3125   train_loss = 0.998\n",
      "2018-06-07T16:46:24.076597: Epoch   3 Batch 1725/3125   train_loss = 0.875\n",
      "2018-06-07T16:46:24.769567: Epoch   3 Batch 1745/3125   train_loss = 0.786\n",
      "2018-06-07T16:46:25.477980: Epoch   3 Batch 1765/3125   train_loss = 0.888\n",
      "2018-06-07T16:46:26.235620: Epoch   3 Batch 1785/3125   train_loss = 1.027\n",
      "2018-06-07T16:46:26.953911: Epoch   3 Batch 1805/3125   train_loss = 0.926\n",
      "2018-06-07T16:46:27.665530: Epoch   3 Batch 1825/3125   train_loss = 0.997\n",
      "2018-06-07T16:46:28.366919: Epoch   3 Batch 1845/3125   train_loss = 1.008\n",
      "2018-06-07T16:46:29.106004: Epoch   3 Batch 1865/3125   train_loss = 0.834\n",
      "2018-06-07T16:46:29.806304: Epoch   3 Batch 1885/3125   train_loss = 0.967\n",
      "2018-06-07T16:46:30.494878: Epoch   3 Batch 1905/3125   train_loss = 0.826\n",
      "2018-06-07T16:46:31.183203: Epoch   3 Batch 1925/3125   train_loss = 0.850\n",
      "2018-06-07T16:46:31.888957: Epoch   3 Batch 1945/3125   train_loss = 0.874\n",
      "2018-06-07T16:46:32.576818: Epoch   3 Batch 1965/3125   train_loss = 0.829\n",
      "2018-06-07T16:46:33.349507: Epoch   3 Batch 1985/3125   train_loss = 0.828\n",
      "2018-06-07T16:46:34.079868: Epoch   3 Batch 2005/3125   train_loss = 0.935\n",
      "2018-06-07T16:46:34.832438: Epoch   3 Batch 2025/3125   train_loss = 0.960\n",
      "2018-06-07T16:46:35.527380: Epoch   3 Batch 2045/3125   train_loss = 0.786\n",
      "2018-06-07T16:46:36.221028: Epoch   3 Batch 2065/3125   train_loss = 0.753\n",
      "2018-06-07T16:46:36.923439: Epoch   3 Batch 2085/3125   train_loss = 1.018\n",
      "2018-06-07T16:46:37.616206: Epoch   3 Batch 2105/3125   train_loss = 0.840\n",
      "2018-06-07T16:46:38.318049: Epoch   3 Batch 2125/3125   train_loss = 0.956\n",
      "2018-06-07T16:46:39.011555: Epoch   3 Batch 2145/3125   train_loss = 1.042\n",
      "2018-06-07T16:46:39.697384: Epoch   3 Batch 2165/3125   train_loss = 0.840\n",
      "2018-06-07T16:46:40.384557: Epoch   3 Batch 2185/3125   train_loss = 0.943\n",
      "2018-06-07T16:46:41.072715: Epoch   3 Batch 2205/3125   train_loss = 0.981\n",
      "2018-06-07T16:46:41.755366: Epoch   3 Batch 2225/3125   train_loss = 0.829\n",
      "2018-06-07T16:46:42.441932: Epoch   3 Batch 2245/3125   train_loss = 0.780\n",
      "2018-06-07T16:46:43.146450: Epoch   3 Batch 2265/3125   train_loss = 0.872\n",
      "2018-06-07T16:46:43.839419: Epoch   3 Batch 2285/3125   train_loss = 1.023\n",
      "2018-06-07T16:46:44.534424: Epoch   3 Batch 2305/3125   train_loss = 0.855\n",
      "2018-06-07T16:46:45.233271: Epoch   3 Batch 2325/3125   train_loss = 0.825\n",
      "2018-06-07T16:46:45.928504: Epoch   3 Batch 2345/3125   train_loss = 0.913\n",
      "2018-06-07T16:46:46.614903: Epoch   3 Batch 2365/3125   train_loss = 0.711\n",
      "2018-06-07T16:46:47.303086: Epoch   3 Batch 2385/3125   train_loss = 0.952\n",
      "2018-06-07T16:46:48.099851: Epoch   3 Batch 2405/3125   train_loss = 0.919\n",
      "2018-06-07T16:46:48.828145: Epoch   3 Batch 2425/3125   train_loss = 0.834\n",
      "2018-06-07T16:46:49.530391: Epoch   3 Batch 2445/3125   train_loss = 0.960\n",
      "2018-06-07T16:46:50.291742: Epoch   3 Batch 2465/3125   train_loss = 0.783\n",
      "2018-06-07T16:46:51.042444: Epoch   3 Batch 2485/3125   train_loss = 0.869\n",
      "2018-06-07T16:46:51.739498: Epoch   3 Batch 2505/3125   train_loss = 0.878\n",
      "2018-06-07T16:46:52.453446: Epoch   3 Batch 2525/3125   train_loss = 0.843\n",
      "2018-06-07T16:46:53.165985: Epoch   3 Batch 2545/3125   train_loss = 0.987\n",
      "2018-06-07T16:46:53.853812: Epoch   3 Batch 2565/3125   train_loss = 0.882\n",
      "2018-06-07T16:46:54.575130: Epoch   3 Batch 2585/3125   train_loss = 0.750\n",
      "2018-06-07T16:46:55.283712: Epoch   3 Batch 2605/3125   train_loss = 0.834\n",
      "2018-06-07T16:46:55.980715: Epoch   3 Batch 2625/3125   train_loss = 1.005\n",
      "2018-06-07T16:46:56.675772: Epoch   3 Batch 2645/3125   train_loss = 0.907\n",
      "2018-06-07T16:46:57.339573: Epoch   3 Batch 2665/3125   train_loss = 0.983\n",
      "2018-06-07T16:46:58.098321: Epoch   3 Batch 2685/3125   train_loss = 0.915\n",
      "2018-06-07T16:46:58.782154: Epoch   3 Batch 2705/3125   train_loss = 0.803\n",
      "2018-06-07T16:46:59.542774: Epoch   3 Batch 2725/3125   train_loss = 0.954\n",
      "2018-06-07T16:47:00.265141: Epoch   3 Batch 2745/3125   train_loss = 0.915\n",
      "2018-06-07T16:47:00.979872: Epoch   3 Batch 2765/3125   train_loss = 0.842\n",
      "2018-06-07T16:47:01.647632: Epoch   3 Batch 2785/3125   train_loss = 0.928\n",
      "2018-06-07T16:47:02.353091: Epoch   3 Batch 2805/3125   train_loss = 0.807\n",
      "2018-06-07T16:47:03.053819: Epoch   3 Batch 2825/3125   train_loss = 0.858\n",
      "2018-06-07T16:47:03.822746: Epoch   3 Batch 2845/3125   train_loss = 0.839\n",
      "2018-06-07T16:47:04.627407: Epoch   3 Batch 2865/3125   train_loss = 0.787\n",
      "2018-06-07T16:47:05.311425: Epoch   3 Batch 2885/3125   train_loss = 0.943\n",
      "2018-06-07T16:47:06: Epoch   3 Batch 2905/3125   train_loss = 0.966\n",
      "2018-06-07T16:47:06.674856: Epoch   3 Batch 2925/3125   train_loss = 0.884\n",
      "2018-06-07T16:47:07.338598: Epoch   3 Batch 2945/3125   train_loss = 0.975\n",
      "2018-06-07T16:47:08.021757: Epoch   3 Batch 2965/3125   train_loss = 0.939\n",
      "2018-06-07T16:47:08.698304: Epoch   3 Batch 2985/3125   train_loss = 0.807\n",
      "2018-06-07T16:47:09.431122: Epoch   3 Batch 3005/3125   train_loss = 0.844\n",
      "2018-06-07T16:47:10.116427: Epoch   3 Batch 3025/3125   train_loss = 0.928\n",
      "2018-06-07T16:47:10.782795: Epoch   3 Batch 3045/3125   train_loss = 0.917\n",
      "2018-06-07T16:47:11.587116: Epoch   3 Batch 3065/3125   train_loss = 0.868\n",
      "2018-06-07T16:47:12.280195: Epoch   3 Batch 3085/3125   train_loss = 0.886\n",
      "2018-06-07T16:47:12.975825: Epoch   3 Batch 3105/3125   train_loss = 0.925\n",
      "2018-06-07T16:47:13.808491: Epoch   3 Batch   17/781   test_loss = 0.937\n",
      "2018-06-07T16:47:13.982981: Epoch   3 Batch   37/781   test_loss = 0.923\n",
      "2018-06-07T16:47:14.154863: Epoch   3 Batch   57/781   test_loss = 0.934\n",
      "2018-06-07T16:47:14.335505: Epoch   3 Batch   77/781   test_loss = 0.886\n",
      "2018-06-07T16:47:14.511554: Epoch   3 Batch   97/781   test_loss = 0.775\n",
      "2018-06-07T16:47:14.688044: Epoch   3 Batch  117/781   test_loss = 0.971\n",
      "2018-06-07T16:47:14.912696: Epoch   3 Batch  137/781   test_loss = 0.895\n",
      "2018-06-07T16:47:15.112194: Epoch   3 Batch  157/781   test_loss = 0.939\n",
      "2018-06-07T16:47:15.332313: Epoch   3 Batch  177/781   test_loss = 0.857\n",
      "2018-06-07T16:47:15.497426: Epoch   3 Batch  197/781   test_loss = 0.872\n",
      "2018-06-07T16:47:15.659773: Epoch   3 Batch  217/781   test_loss = 0.705\n",
      "2018-06-07T16:47:15.821837: Epoch   3 Batch  237/781   test_loss = 0.777\n",
      "2018-06-07T16:47:15.977016: Epoch   3 Batch  257/781   test_loss = 1.002\n",
      "2018-06-07T16:47:16.124740: Epoch   3 Batch  277/781   test_loss = 0.970\n",
      "2018-06-07T16:47:16.295477: Epoch   3 Batch  297/781   test_loss = 0.974\n",
      "2018-06-07T16:47:16.448668: Epoch   3 Batch  317/781   test_loss = 1.019\n",
      "2018-06-07T16:47:16.604499: Epoch   3 Batch  337/781   test_loss = 0.916\n",
      "2018-06-07T16:47:16.766109: Epoch   3 Batch  357/781   test_loss = 0.905\n",
      "2018-06-07T16:47:16.934597: Epoch   3 Batch  377/781   test_loss = 0.941\n",
      "2018-06-07T16:47:17.086908: Epoch   3 Batch  397/781   test_loss = 0.954\n",
      "2018-06-07T16:47:17.279570: Epoch   3 Batch  417/781   test_loss = 0.838\n",
      "2018-06-07T16:47:17.440339: Epoch   3 Batch  437/781   test_loss = 0.808\n",
      "2018-06-07T16:47:17.600443: Epoch   3 Batch  457/781   test_loss = 0.714\n",
      "2018-06-07T16:47:17.758672: Epoch   3 Batch  477/781   test_loss = 0.887\n",
      "2018-06-07T16:47:17.932534: Epoch   3 Batch  497/781   test_loss = 0.822\n",
      "2018-06-07T16:47:18.080330: Epoch   3 Batch  517/781   test_loss = 0.819\n",
      "2018-06-07T16:47:18.242062: Epoch   3 Batch  537/781   test_loss = 0.817\n",
      "2018-06-07T16:47:18.414432: Epoch   3 Batch  557/781   test_loss = 1.000\n",
      "2018-06-07T16:47:18.603604: Epoch   3 Batch  577/781   test_loss = 0.894\n",
      "2018-06-07T16:47:18.753515: Epoch   3 Batch  597/781   test_loss = 0.874\n",
      "2018-06-07T16:47:18.920023: Epoch   3 Batch  617/781   test_loss = 0.867\n",
      "2018-06-07T16:47:19.065902: Epoch   3 Batch  637/781   test_loss = 0.780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-07T16:47:19.218881: Epoch   3 Batch  657/781   test_loss = 0.998\n",
      "2018-06-07T16:47:19.360945: Epoch   3 Batch  677/781   test_loss = 0.927\n",
      "2018-06-07T16:47:19.532014: Epoch   3 Batch  697/781   test_loss = 0.935\n",
      "2018-06-07T16:47:19.673769: Epoch   3 Batch  717/781   test_loss = 0.875\n",
      "2018-06-07T16:47:19.855802: Epoch   3 Batch  737/781   test_loss = 0.749\n",
      "2018-06-07T16:47:20.014618: Epoch   3 Batch  757/781   test_loss = 1.071\n",
      "2018-06-07T16:47:20.168688: Epoch   3 Batch  777/781   test_loss = 0.944\n",
      "2018-06-07T16:47:20.905446: Epoch   4 Batch    0/3125   train_loss = 1.004\n",
      "2018-06-07T16:47:21.632883: Epoch   4 Batch   20/3125   train_loss = 0.870\n",
      "2018-06-07T16:47:22.333516: Epoch   4 Batch   40/3125   train_loss = 0.911\n",
      "2018-06-07T16:47:23.019127: Epoch   4 Batch   60/3125   train_loss = 0.746\n",
      "2018-06-07T16:47:23.780896: Epoch   4 Batch   80/3125   train_loss = 0.859\n",
      "2018-06-07T16:47:24.483343: Epoch   4 Batch  100/3125   train_loss = 0.926\n",
      "2018-06-07T16:47:25.159218: Epoch   4 Batch  120/3125   train_loss = 0.948\n",
      "2018-06-07T16:47:25.853540: Epoch   4 Batch  140/3125   train_loss = 0.944\n",
      "2018-06-07T16:47:26.653757: Epoch   4 Batch  160/3125   train_loss = 0.780\n",
      "2018-06-07T16:47:27.347948: Epoch   4 Batch  180/3125   train_loss = 0.883\n",
      "2018-06-07T16:47:28.027914: Epoch   4 Batch  200/3125   train_loss = 1.038\n",
      "2018-06-07T16:47:28.758876: Epoch   4 Batch  220/3125   train_loss = 0.900\n",
      "2018-06-07T16:47:29.435942: Epoch   4 Batch  240/3125   train_loss = 0.992\n",
      "2018-06-07T16:47:30.123622: Epoch   4 Batch  260/3125   train_loss = 0.944\n",
      "2018-06-07T16:47:30.799226: Epoch   4 Batch  280/3125   train_loss = 0.940\n",
      "2018-06-07T16:47:31.527755: Epoch   4 Batch  300/3125   train_loss = 1.092\n",
      "2018-06-07T16:47:32.246904: Epoch   4 Batch  320/3125   train_loss = 0.979\n",
      "2018-06-07T16:47:32.933014: Epoch   4 Batch  340/3125   train_loss = 0.746\n",
      "2018-06-07T16:47:33.597665: Epoch   4 Batch  360/3125   train_loss = 0.862\n",
      "2018-06-07T16:47:34.450061: Epoch   4 Batch  380/3125   train_loss = 0.856\n",
      "2018-06-07T16:47:35.134182: Epoch   4 Batch  400/3125   train_loss = 0.822\n",
      "2018-06-07T16:47:35.808306: Epoch   4 Batch  420/3125   train_loss = 0.824\n",
      "2018-06-07T16:47:36.514862: Epoch   4 Batch  440/3125   train_loss = 0.864\n",
      "2018-06-07T16:47:37.189702: Epoch   4 Batch  460/3125   train_loss = 0.883\n",
      "2018-06-07T16:47:37.853331: Epoch   4 Batch  480/3125   train_loss = 0.955\n",
      "2018-06-07T16:47:38.547717: Epoch   4 Batch  500/3125   train_loss = 0.654\n",
      "2018-06-07T16:47:39.265104: Epoch   4 Batch  520/3125   train_loss = 0.915\n",
      "2018-06-07T16:47:39.998422: Epoch   4 Batch  540/3125   train_loss = 0.771\n",
      "2018-06-07T16:47:40.734860: Epoch   4 Batch  560/3125   train_loss = 0.970\n",
      "2018-06-07T16:47:41.440553: Epoch   4 Batch  580/3125   train_loss = 0.937\n",
      "2018-06-07T16:47:42.121815: Epoch   4 Batch  600/3125   train_loss = 0.905\n",
      "2018-06-07T16:47:42.803175: Epoch   4 Batch  620/3125   train_loss = 0.928\n",
      "2018-06-07T16:47:43.481169: Epoch   4 Batch  640/3125   train_loss = 0.858\n",
      "2018-06-07T16:47:44.203798: Epoch   4 Batch  660/3125   train_loss = 0.889\n",
      "2018-06-07T16:47:44.903176: Epoch   4 Batch  680/3125   train_loss = 0.937\n",
      "2018-06-07T16:47:45.585439: Epoch   4 Batch  700/3125   train_loss = 0.940\n",
      "2018-06-07T16:47:46.268196: Epoch   4 Batch  720/3125   train_loss = 0.787\n",
      "2018-06-07T16:47:46.997462: Epoch   4 Batch  740/3125   train_loss = 0.907\n",
      "2018-06-07T16:47:47.725336: Epoch   4 Batch  760/3125   train_loss = 0.826\n",
      "2018-06-07T16:47:48.423782: Epoch   4 Batch  780/3125   train_loss = 0.876\n",
      "2018-06-07T16:47:49.229184: Epoch   4 Batch  800/3125   train_loss = 0.765\n",
      "2018-06-07T16:47:49.916404: Epoch   4 Batch  820/3125   train_loss = 0.846\n",
      "2018-06-07T16:47:50.597680: Epoch   4 Batch  840/3125   train_loss = 0.808\n",
      "2018-06-07T16:47:51.338477: Epoch   4 Batch  860/3125   train_loss = 0.865\n",
      "2018-06-07T16:47:52.015319: Epoch   4 Batch  880/3125   train_loss = 0.764\n",
      "2018-06-07T16:47:52.689998: Epoch   4 Batch  900/3125   train_loss = 0.858\n",
      "2018-06-07T16:47:53.414758: Epoch   4 Batch  920/3125   train_loss = 0.963\n",
      "2018-06-07T16:47:54.103869: Epoch   4 Batch  940/3125   train_loss = 0.882\n",
      "2018-06-07T16:47:54.811615: Epoch   4 Batch  960/3125   train_loss = 0.928\n",
      "2018-06-07T16:47:55.548227: Epoch   4 Batch  980/3125   train_loss = 0.950\n",
      "2018-06-07T16:47:56.227474: Epoch   4 Batch 1000/3125   train_loss = 0.984\n",
      "2018-06-07T16:47:56.913373: Epoch   4 Batch 1020/3125   train_loss = 0.906\n",
      "2018-06-07T16:47:57.596132: Epoch   4 Batch 1040/3125   train_loss = 0.816\n",
      "2018-06-07T16:47:58.287581: Epoch   4 Batch 1060/3125   train_loss = 0.952\n",
      "2018-06-07T16:47:58.985529: Epoch   4 Batch 1080/3125   train_loss = 0.932\n",
      "2018-06-07T16:47:59.698207: Epoch   4 Batch 1100/3125   train_loss = 0.876\n",
      "2018-06-07T16:48:00.426692: Epoch   4 Batch 1120/3125   train_loss = 0.908\n",
      "2018-06-07T16:48:01.155242: Epoch   4 Batch 1140/3125   train_loss = 0.900\n",
      "2018-06-07T16:48:01.887272: Epoch   4 Batch 1160/3125   train_loss = 0.829\n",
      "2018-06-07T16:48:02.583019: Epoch   4 Batch 1180/3125   train_loss = 0.844\n",
      "2018-06-07T16:48:03.274153: Epoch   4 Batch 1200/3125   train_loss = 0.994\n",
      "2018-06-07T16:48:03.943802: Epoch   4 Batch 1220/3125   train_loss = 0.945\n",
      "2018-06-07T16:48:04.691066: Epoch   4 Batch 1240/3125   train_loss = 0.783\n",
      "2018-06-07T16:48:05.361648: Epoch   4 Batch 1260/3125   train_loss = 0.905\n",
      "2018-06-07T16:48:06.077135: Epoch   4 Batch 1280/3125   train_loss = 0.860\n",
      "2018-06-07T16:48:06.749112: Epoch   4 Batch 1300/3125   train_loss = 0.806\n",
      "2018-06-07T16:48:07.506120: Epoch   4 Batch 1320/3125   train_loss = 0.804\n",
      "2018-06-07T16:48:08.200881: Epoch   4 Batch 1340/3125   train_loss = 0.747\n",
      "2018-06-07T16:48:08.991466: Epoch   4 Batch 1360/3125   train_loss = 0.837\n",
      "2018-06-07T16:48:09.685614: Epoch   4 Batch 1380/3125   train_loss = 0.818\n",
      "2018-06-07T16:48:10.377543: Epoch   4 Batch 1400/3125   train_loss = 0.928\n",
      "2018-06-07T16:48:11.067592: Epoch   4 Batch 1420/3125   train_loss = 0.905\n",
      "2018-06-07T16:48:11.810055: Epoch   4 Batch 1440/3125   train_loss = 0.791\n",
      "2018-06-07T16:48:12.516842: Epoch   4 Batch 1460/3125   train_loss = 0.856\n",
      "2018-06-07T16:48:13.229230: Epoch   4 Batch 1480/3125   train_loss = 0.887\n",
      "2018-06-07T16:48:13.924231: Epoch   4 Batch 1500/3125   train_loss = 0.867\n",
      "2018-06-07T16:48:14.611256: Epoch   4 Batch 1520/3125   train_loss = 0.851\n",
      "2018-06-07T16:48:15.343240: Epoch   4 Batch 1540/3125   train_loss = 0.979\n",
      "2018-06-07T16:48:16.015104: Epoch   4 Batch 1560/3125   train_loss = 0.790\n",
      "2018-06-07T16:48:16.699037: Epoch   4 Batch 1580/3125   train_loss = 0.957\n",
      "2018-06-07T16:48:17.401866: Epoch   4 Batch 1600/3125   train_loss = 0.802\n",
      "2018-06-07T16:48:18.089368: Epoch   4 Batch 1620/3125   train_loss = 0.783\n",
      "2018-06-07T16:48:18.760490: Epoch   4 Batch 1640/3125   train_loss = 0.933\n",
      "2018-06-07T16:48:19.442391: Epoch   4 Batch 1660/3125   train_loss = 0.994\n",
      "2018-06-07T16:48:20.122169: Epoch   4 Batch 1680/3125   train_loss = 0.898\n",
      "2018-06-07T16:48:20.818822: Epoch   4 Batch 1700/3125   train_loss = 0.808\n",
      "2018-06-07T16:48:21.600449: Epoch   4 Batch 1720/3125   train_loss = 0.921\n",
      "2018-06-07T16:48:22.300741: Epoch   4 Batch 1740/3125   train_loss = 0.934\n",
      "2018-06-07T16:48:22.999429: Epoch   4 Batch 1760/3125   train_loss = 0.902\n",
      "2018-06-07T16:48:23.738502: Epoch   4 Batch 1780/3125   train_loss = 0.912\n",
      "2018-06-07T16:48:24.428501: Epoch   4 Batch 1800/3125   train_loss = 0.852\n",
      "2018-06-07T16:48:25.101965: Epoch   4 Batch 1820/3125   train_loss = 0.840\n",
      "2018-06-07T16:48:25.783771: Epoch   4 Batch 1840/3125   train_loss = 0.923\n",
      "2018-06-07T16:48:26.482223: Epoch   4 Batch 1860/3125   train_loss = 0.925\n",
      "2018-06-07T16:48:27.168715: Epoch   4 Batch 1880/3125   train_loss = 0.880\n",
      "2018-06-07T16:48:27.883962: Epoch   4 Batch 1900/3125   train_loss = 0.739\n",
      "2018-06-07T16:48:28.637983: Epoch   4 Batch 1920/3125   train_loss = 0.869\n",
      "2018-06-07T16:48:29.316996: Epoch   4 Batch 1940/3125   train_loss = 0.797\n",
      "2018-06-07T16:48:30.041675: Epoch   4 Batch 1960/3125   train_loss = 0.751\n",
      "2018-06-07T16:48:30.716609: Epoch   4 Batch 1980/3125   train_loss = 0.861\n",
      "2018-06-07T16:48:31.416848: Epoch   4 Batch 2000/3125   train_loss = 1.050\n",
      "2018-06-07T16:48:32.126828: Epoch   4 Batch 2020/3125   train_loss = 0.962\n",
      "2018-06-07T16:48:32.846155: Epoch   4 Batch 2040/3125   train_loss = 0.793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-07T16:48:33.529821: Epoch   4 Batch 2060/3125   train_loss = 0.827\n",
      "2018-06-07T16:48:34.239683: Epoch   4 Batch 2080/3125   train_loss = 1.036\n",
      "2018-06-07T16:48:34.991403: Epoch   4 Batch 2100/3125   train_loss = 0.818\n",
      "2018-06-07T16:48:35.657587: Epoch   4 Batch 2120/3125   train_loss = 0.820\n",
      "2018-06-07T16:48:36.330870: Epoch   4 Batch 2140/3125   train_loss = 0.911\n",
      "2018-06-07T16:48:37.016709: Epoch   4 Batch 2160/3125   train_loss = 0.849\n",
      "2018-06-07T16:48:37.700545: Epoch   4 Batch 2180/3125   train_loss = 0.932\n",
      "2018-06-07T16:48:38.373518: Epoch   4 Batch 2200/3125   train_loss = 0.790\n",
      "2018-06-07T16:48:39.063198: Epoch   4 Batch 2220/3125   train_loss = 0.800\n",
      "2018-06-07T16:48:39.736864: Epoch   4 Batch 2240/3125   train_loss = 0.830\n",
      "2018-06-07T16:48:40.421485: Epoch   4 Batch 2260/3125   train_loss = 0.899\n",
      "2018-06-07T16:48:41.104128: Epoch   4 Batch 2280/3125   train_loss = 0.852\n",
      "2018-06-07T16:48:41.789875: Epoch   4 Batch 2300/3125   train_loss = 0.820\n",
      "2018-06-07T16:48:42.456699: Epoch   4 Batch 2320/3125   train_loss = 0.906\n",
      "2018-06-07T16:48:43.222488: Epoch   4 Batch 2340/3125   train_loss = 0.897\n",
      "2018-06-07T16:48:43.937371: Epoch   4 Batch 2360/3125   train_loss = 0.910\n",
      "2018-06-07T16:48:44.627677: Epoch   4 Batch 2380/3125   train_loss = 0.829\n",
      "2018-06-07T16:48:45.311622: Epoch   4 Batch 2400/3125   train_loss = 0.893\n",
      "2018-06-07T16:48:46.065357: Epoch   4 Batch 2420/3125   train_loss = 0.792\n",
      "2018-06-07T16:48:46.912198: Epoch   4 Batch 2440/3125   train_loss = 0.790\n",
      "2018-06-07T16:48:47.757970: Epoch   4 Batch 2460/3125   train_loss = 0.843\n",
      "2018-06-07T16:48:48.517599: Epoch   4 Batch 2480/3125   train_loss = 0.967\n",
      "2018-06-07T16:48:49.282927: Epoch   4 Batch 2500/3125   train_loss = 0.833\n",
      "2018-06-07T16:48:50.059897: Epoch   4 Batch 2520/3125   train_loss = 0.900\n",
      "2018-06-07T16:48:50.836033: Epoch   4 Batch 2540/3125   train_loss = 0.809\n",
      "2018-06-07T16:48:51.614357: Epoch   4 Batch 2560/3125   train_loss = 0.665\n",
      "2018-06-07T16:48:52.352154: Epoch   4 Batch 2580/3125   train_loss = 0.888\n",
      "2018-06-07T16:48:53.031312: Epoch   4 Batch 2600/3125   train_loss = 0.869\n",
      "2018-06-07T16:48:53.753998: Epoch   4 Batch 2620/3125   train_loss = 0.799\n",
      "2018-06-07T16:48:54.449297: Epoch   4 Batch 2640/3125   train_loss = 0.823\n",
      "2018-06-07T16:48:55.140777: Epoch   4 Batch 2660/3125   train_loss = 1.025\n",
      "2018-06-07T16:48:55.816404: Epoch   4 Batch 2680/3125   train_loss = 0.804\n",
      "2018-06-07T16:48:56.489940: Epoch   4 Batch 2700/3125   train_loss = 0.860\n",
      "2018-06-07T16:48:57.224416: Epoch   4 Batch 2720/3125   train_loss = 0.737\n",
      "2018-06-07T16:48:57.929527: Epoch   4 Batch 2740/3125   train_loss = 0.854\n",
      "2018-06-07T16:48:58.622082: Epoch   4 Batch 2760/3125   train_loss = 0.759\n",
      "2018-06-07T16:48:59.373726: Epoch   4 Batch 2780/3125   train_loss = 0.837\n",
      "2018-06-07T16:49:00.261018: Epoch   4 Batch 2800/3125   train_loss = 1.015\n",
      "2018-06-07T16:49:01.006895: Epoch   4 Batch 2820/3125   train_loss = 1.022\n",
      "2018-06-07T16:49:01.780785: Epoch   4 Batch 2840/3125   train_loss = 0.844\n",
      "2018-06-07T16:49:02.537499: Epoch   4 Batch 2860/3125   train_loss = 0.773\n",
      "2018-06-07T16:49:03.297142: Epoch   4 Batch 2880/3125   train_loss = 0.824\n",
      "2018-06-07T16:49:04.033153: Epoch   4 Batch 2900/3125   train_loss = 0.868\n",
      "2018-06-07T16:49:04.780372: Epoch   4 Batch 2920/3125   train_loss = 0.846\n",
      "2018-06-07T16:49:05.686931: Epoch   4 Batch 2940/3125   train_loss = 0.919\n",
      "2018-06-07T16:49:06.651066: Epoch   4 Batch 2960/3125   train_loss = 0.873\n",
      "2018-06-07T16:49:07.527896: Epoch   4 Batch 2980/3125   train_loss = 0.829\n",
      "2018-06-07T16:49:08.453488: Epoch   4 Batch 3000/3125   train_loss = 0.916\n",
      "2018-06-07T16:49:09.177345: Epoch   4 Batch 3020/3125   train_loss = 1.013\n",
      "2018-06-07T16:49:09.889420: Epoch   4 Batch 3040/3125   train_loss = 0.881\n",
      "2018-06-07T16:49:10.595656: Epoch   4 Batch 3060/3125   train_loss = 0.778\n",
      "2018-06-07T16:49:11.383937: Epoch   4 Batch 3080/3125   train_loss = 1.007\n",
      "2018-06-07T16:49:12.102912: Epoch   4 Batch 3100/3125   train_loss = 1.056\n",
      "2018-06-07T16:49:12.809225: Epoch   4 Batch 3120/3125   train_loss = 0.819\n",
      "2018-06-07T16:49:13.109356: Epoch   4 Batch   16/781   test_loss = 0.840\n",
      "2018-06-07T16:49:13.280148: Epoch   4 Batch   36/781   test_loss = 0.970\n",
      "2018-06-07T16:49:13.446941: Epoch   4 Batch   56/781   test_loss = 0.904\n",
      "2018-06-07T16:49:13.603828: Epoch   4 Batch   76/781   test_loss = 0.992\n",
      "2018-06-07T16:49:13.767525: Epoch   4 Batch   96/781   test_loss = 1.009\n",
      "2018-06-07T16:49:13.959208: Epoch   4 Batch  116/781   test_loss = 0.862\n",
      "2018-06-07T16:49:14.126682: Epoch   4 Batch  136/781   test_loss = 0.797\n",
      "2018-06-07T16:49:14.324861: Epoch   4 Batch  156/781   test_loss = 0.903\n",
      "2018-06-07T16:49:14.509526: Epoch   4 Batch  176/781   test_loss = 0.877\n",
      "2018-06-07T16:49:14.685525: Epoch   4 Batch  196/781   test_loss = 0.791\n",
      "2018-06-07T16:49:14.870191: Epoch   4 Batch  216/781   test_loss = 0.961\n",
      "2018-06-07T16:49:15.036143: Epoch   4 Batch  236/781   test_loss = 0.812\n",
      "2018-06-07T16:49:15.225053: Epoch   4 Batch  256/781   test_loss = 0.832\n",
      "2018-06-07T16:49:15.399583: Epoch   4 Batch  276/781   test_loss = 1.114\n",
      "2018-06-07T16:49:15.564114: Epoch   4 Batch  296/781   test_loss = 0.806\n",
      "2018-06-07T16:49:15.796217: Epoch   4 Batch  316/781   test_loss = 0.843\n",
      "2018-06-07T16:49:16.056468: Epoch   4 Batch  336/781   test_loss = 0.797\n",
      "2018-06-07T16:49:16.279126: Epoch   4 Batch  356/781   test_loss = 0.862\n",
      "2018-06-07T16:49:16.439567: Epoch   4 Batch  376/781   test_loss = 0.870\n",
      "2018-06-07T16:49:16.601233: Epoch   4 Batch  396/781   test_loss = 0.879\n",
      "2018-06-07T16:49:16.747519: Epoch   4 Batch  416/781   test_loss = 0.934\n",
      "2018-06-07T16:49:16.914322: Epoch   4 Batch  436/781   test_loss = 0.914\n",
      "2018-06-07T16:49:17.063212: Epoch   4 Batch  456/781   test_loss = 0.722\n",
      "2018-06-07T16:49:17.222603: Epoch   4 Batch  476/781   test_loss = 0.977\n",
      "2018-06-07T16:49:17.378381: Epoch   4 Batch  496/781   test_loss = 0.916\n",
      "2018-06-07T16:49:17.536584: Epoch   4 Batch  516/781   test_loss = 0.807\n",
      "2018-06-07T16:49:17.685484: Epoch   4 Batch  536/781   test_loss = 0.947\n",
      "2018-06-07T16:49:17.855337: Epoch   4 Batch  556/781   test_loss = 0.819\n",
      "2018-06-07T16:49:18.013630: Epoch   4 Batch  576/781   test_loss = 0.982\n",
      "2018-06-07T16:49:18.183513: Epoch   4 Batch  596/781   test_loss = 0.985\n",
      "2018-06-07T16:49:18.342723: Epoch   4 Batch  616/781   test_loss = 0.931\n",
      "2018-06-07T16:49:18.508596: Epoch   4 Batch  636/781   test_loss = 0.845\n",
      "2018-06-07T16:49:18.673941: Epoch   4 Batch  656/781   test_loss = 0.886\n",
      "2018-06-07T16:49:18.992829: Epoch   4 Batch  676/781   test_loss = 1.006\n",
      "2018-06-07T16:49:19.184825: Epoch   4 Batch  696/781   test_loss = 0.869\n",
      "2018-06-07T16:49:19.344093: Epoch   4 Batch  716/781   test_loss = 0.890\n",
      "2018-06-07T16:49:19.533861: Epoch   4 Batch  736/781   test_loss = 1.058\n",
      "2018-06-07T16:49:19.718252: Epoch   4 Batch  756/781   test_loss = 0.824\n",
      "2018-06-07T16:49:19.899358: Epoch   4 Batch  776/781   test_loss = 0.796\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "losses = {'train':[], 'test':[]}\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    \n",
    "    #搜集数据给tensorBoard用\n",
    "    # Keep track of gradient values and sparsity\n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "     \n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        #将数据集分成训练集和测试集，随机种子不固定\n",
    "        train_X,test_X, train_y, test_y = train_test_split(features,  \n",
    "                                                           targets_values,  \n",
    "                                                           test_size = 0.2,  \n",
    "                                                           random_state = 0)  \n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "    \n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
    "                lr: learning_rate}\n",
    "\n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)  #\n",
    "            \n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss))\n",
    "                \n",
    "        #使用测试数据的迭代\n",
    "        for batch_i  in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            \n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
    "\n",
    "            #保存测试损失\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)  #\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // batch_size),\n",
    "                    test_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)  #, global_step=epoch_i\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params((save_dir))\n",
    "\n",
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 370
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 377
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
    "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
    "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
    "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
    "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
    "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
    "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
    "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
    "    #两种不同计算预测评分的方案使用不同的name获取tensor inference\n",
    "#     inference = loaded_graph.get_tensor_by_name(\"inference/inference/BiasAdd:0\")\n",
    "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\") # 之前是MatMul:0 因为inference代码修改了 这里也要修改 感谢网友 @清歌 指出问题\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "    \n",
    "        # Get Tensors from loaded model\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "    \n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "    \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "    \n",
    "        feed = {\n",
    "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              movie_categories: categories,  #x.take(6,1)\n",
    "              movie_titles: titles,  #x.take(5,1)\n",
    "              dropout_keep_prob: 1}\n",
    "    \n",
    "        # Get Prediction\n",
    "        inference_val = sess.run([inference], feed)  \n",
    "    \n",
    "        return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[3.7282224]], dtype=float32)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_movie(234, 1401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "\n",
    "        feed = {\n",
    "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "            movie_categories: categories,  #x.take(6,1)\n",
    "            movie_titles: titles,  #x.take(5,1)\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)  \n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "users_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in users.values:\n",
    "\n",
    "        feed = {\n",
    "            uid: np.reshape(item.take(0), [1, 1]),\n",
    "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
    "            user_age: np.reshape(item.take(2), [1, 1]),\n",
    "            user_job: np.reshape(item.take(3), [1, 1]),\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
    "        users_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k = 20):\n",
    "    \n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
    "\n",
    "        #推荐同类型的电影\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "以下是给您的推荐：\n",
      "1744\n",
      "[1807 'Cool Dry Place, A (1998)' 'Drama']\n",
      "3145\n",
      "[3214 'American Flyers (1985)' 'Drama']\n",
      "771\n",
      "[781 'Stealing Beauty (1996)' 'Drama']\n",
      "1869\n",
      "[1938 'Lost Weekend, The (1945)' 'Drama']\n",
      "1677\n",
      "[1726 'Postman, The (1997)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{771, 1677, 1744, 1869, 3145}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_same_type_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_your_favorite_movie(user_id_val, top_k = 10):\n",
    "\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        #推荐您喜欢的电影\n",
    "        probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])\n",
    "\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     print(sim.shape)\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "    #     sim_norm = probs_norm_similarity.eval()\n",
    "    #     print((-sim_norm[0]).argsort()[0:top_k])\n",
    "    \n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "以下是给您的推荐：\n",
      "552\n",
      "[556 'War Room, The (1993)' 'Documentary']\n",
      "49\n",
      "[50 'Usual Suspects, The (1995)' 'Crime|Thriller']\n",
      "315\n",
      "[318 'Shawshank Redemption, The (1994)' 'Drama']\n",
      "892\n",
      "[904 'Rear Window (1954)' 'Mystery|Thriller']\n",
      "847\n",
      "[858 'Godfather, The (1972)' 'Action|Crime|Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{49, 315, 552, 847, 892}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_your_favorite_movie(234, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def recommend_other_favorite_movie(movie_id_val, top_k = 20):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))\n",
    "        favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]\n",
    "    #     print(normalized_users_matrics.eval().shape)\n",
    "    #     print(probs_user_favorite_similarity.eval()[0][favorite_user_id])\n",
    "    #     print(favorite_user_id.shape)\n",
    "    \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        \n",
    "        print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id-1]))\n",
    "        probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200])\n",
    "        probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "    \n",
    "    #     print(sim.shape)\n",
    "    #     print(np.argmax(sim, 1))\n",
    "        p = np.argmax(sim, 1)\n",
    "        print(\"喜欢看这个电影的人还喜欢看：\")\n",
    "\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = p[random.randrange(top_k)]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "喜欢看这个电影的人是：[[100 'M' 35 17]\n",
      " [3901 'M' 18 14]\n",
      " [3031 'M' 18 4]\n",
      " [2136 'M' 35 18]\n",
      " [4085 'F' 25 6]\n",
      " [3243 'M' 25 12]\n",
      " [883 'F' 35 14]\n",
      " [5767 'M' 25 2]\n",
      " [74 'M' 35 14]\n",
      " [1745 'M' 45 0]\n",
      " [2362 'M' 25 14]\n",
      " [3406 'F' 25 7]\n",
      " [3739 'M' 56 11]\n",
      " [5567 'M' 50 3]\n",
      " [3833 'M' 25 1]\n",
      " [4696 'M' 18 12]\n",
      " [1354 'F' 25 2]\n",
      " [1763 'M' 35 7]\n",
      " [1855 'M' 18 4]\n",
      " [1701 'F' 25 4]]\n",
      "喜欢看这个电影的人还喜欢看：\n"
     ]
    }
   ],
   "source": [
    "recommend_other_favorite_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
